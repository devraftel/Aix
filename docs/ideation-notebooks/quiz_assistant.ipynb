{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Upload Files & save their Metadata to DB\n",
    "# 2. On Generate Button\n",
    "    # - Reterive Assistant\n",
    "    # - Add files to the Assistant and ensure no others are present\n",
    "    # - Assistant uses Knowledge Retrival & Code Intepration\n",
    "    # - Construct the Prompt\n",
    "    # - Generate Questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "from typing import Callable, Literal\n",
    "\n",
    "\n",
    "_: bool = load_dotenv(find_dotenv())  # read local .env file\n",
    "\n",
    "client: OpenAI = OpenAI()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A Class to Manage All Open API Assistant Calls and Functions\n",
    "from openai.types.beta.threads import Run\n",
    "from openai.types.beta.thread import Thread\n",
    "from openai.types.beta.assistant import Assistant\n",
    "from openai.types.beta.assistant_create_params import Tool\n",
    "\n",
    "import time\n",
    "import json\n",
    "\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "from typing import Literal\n",
    "\n",
    "_: bool = load_dotenv(find_dotenv())  # read local .env file\n",
    "\n",
    "client: OpenAI = OpenAI()\n",
    "class GenerateQuestionsAssistant:\n",
    "    def __init__(self, model: str = \"gpt-4-turbo-preview\"):\n",
    "        self.client = OpenAI()\n",
    "        self.model = model\n",
    "        self.assistant: Assistant | None = None\n",
    "        self.thread: Thread | None = None\n",
    "        self.run: Run | None = None\n",
    "\n",
    "    def retrieve_assistant(self, assistant_id: str) -> Assistant:\n",
    "        \"\"\"Retrieve an existing assistant.\"\"\"\n",
    "        assistant = self.client.beta.assistants.retrieve(assistant_id=assistant_id)\n",
    "        return assistant\n",
    "    \n",
    "    def create_assistant(self, instructions: str, name: str, tools: list, file_obj: list[str]) -> Assistant:\n",
    "        \"\"\"Create a new assistant.\"\"\"\n",
    "        print(\"Creating new assistant...\")\n",
    "        self.assistant = self.client.beta.assistants.create(\n",
    "            instructions=instructions,\n",
    "            name=name,\n",
    "            tools=tools,\n",
    "            model=self.model,\n",
    "            file_ids=file_obj\n",
    "        )\n",
    "        return self.assistant\n",
    "\n",
    "    def modifyAssistant(self, assistant_id: str, new_instructions: str, tools: list, file_obj: list[str]) -> Assistant:\n",
    "        \"\"\"Update an existing assistant.\"\"\"\n",
    "        print(\"Updating existing assistant...\")\n",
    "        self.assistant = self.client.beta.assistants.update(\n",
    "            assistant_id=assistant_id,\n",
    "            instructions=new_instructions,\n",
    "            tools=tools,\n",
    "            model=self.model,\n",
    "            file_ids=file_obj\n",
    "        )\n",
    "        return self.assistant\n",
    "    \n",
    "    def list_assistants(self) -> list:\n",
    "        \"\"\"Retrieve a list of assistants.\"\"\"\n",
    "        assistants_list = self.client.beta.assistants.list()\n",
    "        assistants = assistants_list.model_dump()\n",
    "        return assistants['data'] if 'data' in assistants else []\n",
    "\n",
    "\n",
    "    def create_thread(self) -> Thread:\n",
    "        self.thread = self.client.beta.threads.create()\n",
    "        return self.thread\n",
    "\n",
    "    def add_message_to_thread(self, role: Literal['user'], content: str) -> None:\n",
    "        if self.thread is None:\n",
    "            raise ValueError(\"Thread is not set!\")\n",
    "\n",
    "        self.client.beta.threads.messages.create(\n",
    "            thread_id=self.thread.id,\n",
    "            role=role,\n",
    "            content=content\n",
    "        )\n",
    "\n",
    "    def run_assistant(self, instructions: str) -> Run:\n",
    "\n",
    "        if self.assistant is None:\n",
    "            raise ValueError(\n",
    "                \"Assistant is not set. Cannot run assistant without an assistant.\")\n",
    "\n",
    "        if self.thread is None:\n",
    "            raise ValueError(\n",
    "                \"Thread is not set!\")\n",
    "\n",
    "        self.run = self.client.beta.threads.runs.create(\n",
    "            thread_id=self.thread.id,\n",
    "            assistant_id=self.assistant.id,\n",
    "            instructions=instructions\n",
    "        )\n",
    "        return self.run\n",
    "\n",
    "    def wait_for_completion(self, run: Run, thread: Thread):\n",
    "\n",
    "        if self.run is None:\n",
    "            raise ValueError(\"Run is not set!\")\n",
    "\n",
    "        # while run.status in [\"in_progress\", \"queued\"]:\n",
    "        while run.status not in [\"completed\", \"failed\"]:\n",
    "            run_status = self.client.beta.threads.runs.retrieve(\n",
    "                thread_id=thread.id,\n",
    "                run_id=self.run.id\n",
    "            )\n",
    "            time.sleep(1)  # Wait for 3 seconds before checking again\n",
    "\n",
    "            print(f\"Run Status: {run_status.status}\")\n",
    "\n",
    "            if run_status.status == 'completed':\n",
    "                print(\"Run completed.\")\n",
    "                processed_response = self.process_messages()\n",
    "                return processed_response\n",
    "                # break\n",
    "            elif run.status == \"failed\":\n",
    "                print(\"Run failed.\")\n",
    "                break\n",
    "            else:\n",
    "                print(f\"Waiting for the Assistant to process...: {run.status}\")\n",
    "\n",
    "\n",
    "    def process_messages(self):\n",
    "        messages = self.client.beta.threads.messages.list(\n",
    "            thread_id=self.thread.id)\n",
    "        return messages\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from app.openai_sdk.seed_config import GENERATE_QUESTIONS_ASSISTANT_SEED, interpreter, openai_model, assistant_name\n",
    "\n",
    "questions_manager: GenerateQuestionsAssistant = GenerateQuestionsAssistant(model=openai_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating new assistant...\n",
      "\n",
      "----------- quiz_questions_assistant) -----------\n",
      " Assistant(id='asst_35nm0GKkIzy6lghgMWZfBHgB', created_at=1708700505, description=None, file_ids=[], instructions=' \\n                                    You are a quiz question generation assistant that intelligently crafts educational quiz questions across various subjects in JSON format. \\n                                    It should be capable of generating \"single_select_mcq\", \"multi_select_mcq\", \"open_text_question\" types of questions\\n                                    tailored to different difficulty levels from the provided content.\\n\\n                                    It should avoid bias, be sensitive to diverse backgrounds, incorporate user feedback for continuous improvement, and, \\n                                    if possible, support adaptive learning by adjusting questions based on the user\\'s performance. \\n                                    \\n                                    Here\\'s a sample of Generated Questions: [\\n                                        {\\n                                            \"time_limit\": 1, \\n                                            \"question_text\": \"What is Generative AI?\",\\n                                            \"difficulty\": \"medium\",\\n                                            \"question_type\": \"single_select_mcq\",\\n                                            \"points\": 1,\\n                                            \"mcq_options\": [\\n                                                {\"option_text\": \"A type of machine learning that generates new content\", \"is_correct\": True},\\n                                                {\"option_text\": \"A type of machine learning that classifies data\", \"is_correct\": False},\\n                                                {\"option_text\": \"A type of machine learning that optimizes algorithms\", \"is_correct\": False},\\n                                                {\"option_text\": \"A type of machine learning that detects anomalies\", \"is_correct\": False}\\n                                            ]\\n                                        },\\n                                        {\\n                                            \"time_limit\": 1, \\n                                            \"question_text\": \"What is Generative AI?\",\\n                                            \"difficulty\": \"medium\",\\n                                            \"question_type\": \"open_text_question\",\\n                                            \"points\": 1,\\n                                            \"correct_answer\": \"Generative AI is a type of artificial intelligence that is capable of creating new content or data that is similar to the data it was trained on.\"\\n                                        },\\n                                        {\\n                                            \"time_limit\": 2, \\n                                            \"question_text\": \"Select All Incorrect About Generative AI?\",\\n                                            \"difficulty\": \"medium\",\\n                                            \"question_type\": \"multi_select_mcq\",\\n                                            \"points\": 1,\\n                                            \"mcq_options\": [\\n                                                {\"option_text\": \"A type of machine learning that generates new content\", \"is_correct\": False},\\n                                                {\"option_text\": \"A type of machine learning that classifies data\", \"is_correct\": True},\\n                                                {\"option_text\": \"A type of machine learning that optimizes algorithms\",\"is_correct\": True},\\n                                                {\"option_text\": \"A type of machine learning that detects anomalies\", \"is_correct\": True}\\n                                            ]\\n                                        }\\n                                    ]\\n                                    Share the generated questions in JSON format.\\n                                    ', metadata={}, model='gpt-3.5-turbo-0125', name='Quiz Questions Generation Assistant', object='assistant', tools=[ToolCodeInterpreter(type='code_interpreter')])\n",
      "\n",
      "----------- quiz_questions_assistant.id) -----------\n",
      " asst_35nm0GKkIzy6lghgMWZfBHgB\n"
     ]
    }
   ],
   "source": [
    "# Create a New Assistant\n",
    "quiz_questions_assistant= questions_manager.create_assistant(tools=[interpreter], instructions=GENERATE_QUESTIONS_ASSISTANT_SEED, name=assistant_name, file_obj=[])\n",
    "\n",
    "print(\"\\n----------- quiz_questions_assistant) -----------\\n\", quiz_questions_assistant)\n",
    "print(\"\\n----------- quiz_questions_assistant.id) -----------\\n\", quiz_questions_assistant.id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Retrive The Assistant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "retrieved the assistant Assistant(id='asst_35nm0GKkIzy6lghgMWZfBHgB', created_at=1708700505, description=None, file_ids=[], instructions=' \\n                                    You are a quiz question generation assistant that intelligently crafts educational quiz questions across various subjects in JSON format. \\n                                    It should be capable of generating \"single_select_mcq\", \"multi_select_mcq\", \"open_text_question\" types of questions\\n                                    tailored to different difficulty levels from the provided content.\\n\\n                                    It should avoid bias, be sensitive to diverse backgrounds, incorporate user feedback for continuous improvement, and, \\n                                    if possible, support adaptive learning by adjusting questions based on the user\\'s performance. \\n                                    \\n                                    Here\\'s a sample of Generated Questions: [\\n                                        {\\n                                            \"time_limit\": 1, \\n                                            \"question_text\": \"What is Generative AI?\",\\n                                            \"difficulty\": \"medium\",\\n                                            \"question_type\": \"single_select_mcq\",\\n                                            \"points\": 1,\\n                                            \"mcq_options\": [\\n                                                {\"option_text\": \"A type of machine learning that generates new content\", \"is_correct\": True},\\n                                                {\"option_text\": \"A type of machine learning that classifies data\", \"is_correct\": False},\\n                                                {\"option_text\": \"A type of machine learning that optimizes algorithms\", \"is_correct\": False},\\n                                                {\"option_text\": \"A type of machine learning that detects anomalies\", \"is_correct\": False}\\n                                            ]\\n                                        },\\n                                        {\\n                                            \"time_limit\": 1, \\n                                            \"question_text\": \"What is Generative AI?\",\\n                                            \"difficulty\": \"medium\",\\n                                            \"question_type\": \"open_text_question\",\\n                                            \"points\": 1,\\n                                            \"correct_answer\": \"Generative AI is a type of artificial intelligence that is capable of creating new content or data that is similar to the data it was trained on.\"\\n                                        },\\n                                        {\\n                                            \"time_limit\": 2, \\n                                            \"question_text\": \"Select All Incorrect About Generative AI?\",\\n                                            \"difficulty\": \"medium\",\\n                                            \"question_type\": \"multi_select_mcq\",\\n                                            \"points\": 1,\\n                                            \"mcq_options\": [\\n                                                {\"option_text\": \"A type of machine learning that generates new content\", \"is_correct\": False},\\n                                                {\"option_text\": \"A type of machine learning that classifies data\", \"is_correct\": True},\\n                                                {\"option_text\": \"A type of machine learning that optimizes algorithms\",\"is_correct\": True},\\n                                                {\"option_text\": \"A type of machine learning that detects anomalies\", \"is_correct\": True}\\n                                            ]\\n                                        }\\n                                    ]\\n                                    Share the generated questions in JSON format.\\n                                    ', metadata={}, model='gpt-3.5-turbo-0125', name='Quiz Questions Generation Assistant', object='assistant', tools=[ToolCodeInterpreter(type='code_interpreter')])\n"
     ]
    }
   ],
   "source": [
    "# retrieved the assistant\n",
    "assistant_retrieved = questions_manager.retrieve_assistant(quiz_questions_assistant.id)\n",
    "print(\"retrieved the assistant\", assistant_retrieved)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Create a Thread"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "thread = questions_manager.create_thread()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Add a Message to a Thread"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_prompt = \"\"\"Create a 30 minutes \"Generative AI\" that will have 10 Questions. The Questions type will include: 1. \"single_select_mcq\" 2. \"multi_select_mcq\" and \"open_text_question\". The Quiz will be easy and the questions generated shall follow: \"Create a quiz about Applications of Generative AI for Businesses. It will conver fundamental concepts about it. The Content from which Quiz Shall be generated is: \"Chapter 1. Generative AI Use Cases,\n",
    "Fundamentals, and Project Life Cycle\n",
    "In this chapter, you will see some generative AI tasks and use cases in\n",
    "action, gain an understanding of generative foundation models, and explore\n",
    "a typical generative AI project life cycle. The use cases and tasks you’ll see\n",
    "in this chapter include intelligent search, automated customer-support\n",
    "chatbot, dialog summarization, not-safe-for-work (NSFW) content\n",
    "moderation, personalized product videos, source code generation, and\n",
    "others.\n",
    "You will also learn a few of the generative AI service and hardware options\n",
    "from Amazon Web Services (AWS) including Amazon Bedrock, Amazon\n",
    "SageMaker, Amazon CodeWhisperer, AWS Trainium, and AWS Inferentia.\n",
    "These service and hardware options provide great flexibility when building\n",
    "your end-to-end, context-aware, multimodal reasoning applications with\n",
    "generative AI on AWS.\n",
    "Let’s explore some common use cases and tasks for generative AI.\n",
    "Use Cases and Tasks\n",
    "Similar to deep learning, generative AI is a general-purpose technology\n",
    "used for multiple purposes across many industries and customer segments.\n",
    "There are many types of multimodal generative AI tasks. We’ve included a\n",
    "list of the most common generative tasks and associated example use cases:\n",
    "Text summarization\n",
    "Produce a shorter version of a piece of text while retaining the main\n",
    "ideas. Examples include summarizing a news article, legal document, or\n",
    "financial report into a smaller number of words or paragraphs for faster\n",
    "consumption. Often, summarization is used on customer support\n",
    "conversations to provide a quick overview of the interaction between a\n",
    "customer and support representative.\n",
    "Rewriting\n",
    "Modify the wording of a piece of text to adapt to a different audience,\n",
    "formality, or tone. For example, you can convert a formal legal\n",
    "document into a less formal document using less legal terms to appeal\n",
    "to a nonlegal audience.\n",
    "Information extraction\n",
    "Extract information from documents such as names, addresses, events,\n",
    "or numeric data or numbers. For example, converting an email into a\n",
    "purchase order in an enterprise resource planning (ERP) system like\n",
    "SAP.\n",
    "Question answering (QA) and visual question answering (VQA)\n",
    "Ask questions directly against a set of documents, images, videos, or\n",
    "audio clips. For example, you can set up an internal, employee-facing\n",
    "chatbot to answer questions about human resources and benefits\n",
    "documents.\n",
    "Detecting toxic or harmful content\n",
    "An extension to the question-answer task, you can ask a generative\n",
    "model if a set of text, images, videos, or audio clips contains any\n",
    "toxicity or harmful content.\n",
    "Classification and content moderation\n",
    "Assign a category to a given piece of content such as a document,\n",
    "image, video, or audio clip. For example, deleting email spam, filtering\n",
    "out inappropriate images, or labeling incoming, text-based customer\u0002support tickets.\n",
    "Conversational interface\n",
    "Handle multiturn conversations to accomplish tasks through a chat-like\n",
    "interface. Examples include chatbots for self-service customer support\n",
    "or mental health therapy sessions.\n",
    "Translation\n",
    "One of the earliest use cases for generative AI is language translation.\n",
    "Consider, for example, that the publisher of this book wants to release a\n",
    "German translation to help expand the book’s reach. Or perhaps you\n",
    "may want to convert the Python-based examples to Java to work within\n",
    "your existing Java-based enterprise application.\n",
    "Source code generation\n",
    "Create source code from natural language code comments—or even a\n",
    "hand-drawn sketch, as shown in Figure 1-1. Here, an HTML- and\n",
    "JavaScript-based website is generated from a UI sketch scribbled on the\n",
    "back of a restaurant napkin.\n",
    "Figure 1-1. Generating UI code from hand-drawn sketch\n",
    "Reasoning\n",
    "Reason through a problem to discover potential new solutions, trade\u0002offs, or latent details. For example, consider a CFO who provides an\n",
    "audio-based quarterly financial report to investors as well as a more\u0002detailed written report. By reasoning through these different media\n",
    "formats together, the model may discover some conclusions about the\n",
    "company’s health not directly mentioned in the audio or stated in the\n",
    "text.\n",
    "Mask personally identifiable information (PII)\n",
    "You can use generative models to mask personally identifiable\n",
    "information from a given corpus of text. This is useful for many use\n",
    "cases where you are working with sensitive data and wish to remove PII\n",
    "data from your workflows.\n",
    "Personalized marketing and ads\n",
    "Generate personalized product descriptions, videos, or ads based on user\n",
    "profile features. Consider an ecommerce website that wants to generate\n",
    "a personalized description for each product based on the logged-in\n",
    "user’s age or family situation. You could also generate personalized\n",
    "product images that include mature adults, adults with children, or\n",
    "children themselves to better appeal to the logged-in user’s\n",
    "demographic, as shown in Figure 1-2.\n",
    "Figure 1-2. Personalized marketing\n",
    "In this case, each user of the service would potentially see a unique and\n",
    "highly personalized image and description for the same product. This\n",
    "could ultimately lead to more product clicks and higher sales.\n",
    "In each of these generative use cases and tasks, a model creates content that\n",
    "approximates a human’s understanding of language. This is truly amazing\n",
    "and is made possible by a neural network architecture called the\n",
    "transformer, which you will learn in Chapter 3.\n",
    "In the next section, you will learn how to access foundation models through\n",
    "model hubs.\n",
    "Foundation Models and Model Hubs\n",
    "Foundation models are very large and complex neural network models\n",
    "consisting of billions of parameters (a.k.a. weights). The model parameters\n",
    "are learned during the training phase—often called pretraining. Foundation\n",
    "models are trained on massive amounts of training data—typically over a\n",
    "period of many weeks and months using large, distributed clusters of CPUs\n",
    "and graphics processing units (GPUs). After learning billions of parameters,\n",
    "these foundation models can represent complex entities such as human\n",
    "language, images, videos, and audio clips.\n",
    "In most cases, you will start your generative AI projects with an existing\n",
    "foundation model from a model hub such as Hugging Face Model Hub,\n",
    "PyTorch Hub, or Amazon SageMaker JumpStart. A model hub is a\n",
    "collection of models that typically contains detailed model descriptions\n",
    "including the use cases that they address.\n",
    "Throughout this book, we will use Hugging Face Model Hub and\n",
    "SageMaker JumpStart to access foundation models like Llama 2 from Meta\n",
    "(Facebook) and Falcon from the Technology Innovation Institute (TII) and\n",
    "FLAN-T5 from Google. You will dive deeper into model hubs and\n",
    "foundation models in Chapter 3.\n",
    "Next, you’ll see a typical generative AI project life cycle that roughly\n",
    "follows the outline of the rest of this book.\n",
    "Generative AI Project Life Cycle\n",
    "While there is no definitive project life cycle for generative AI projects, the\n",
    "framework shown in Figure 1-3 can help guide you through the most\n",
    "important parts of your generative AI application journey. Throughout the\n",
    "book, you will gain intuition, learn to avoid potential difficulties, and\n",
    "improve your decision making at each step in the journey.\n",
    "Figure 1-3. Generative AI project life cycle framework\n",
    "Let’s dive into each component of the life cycle shown in Figure 1-3:\n",
    "Identify use case.\n",
    "As with any project, you first want to define your scope, including the\n",
    "specific generative use case and task that you plan to address with your\n",
    "generative AI application. We recommend that you start with a single,\n",
    "well-documented generative use case. This will help you get familiar\n",
    "with the environment and understand the power—and limitations—of\n",
    "these models without trying to optimize the model for different tasks at\n",
    "the same time. While these models are capable of carrying out multiple\n",
    "tasks, it’s a bit more difficult to evaluate and optimize the model across\n",
    "multiple tasks to start.\n",
    "Experiment and select.\n",
    "Generative AI models are capable of carrying out many different tasks\n",
    "with great success. However, you will need to decide if an existing\n",
    "foundation model is suitable for your application needs. In Chapter 2,\n",
    "you will learn how to work with these existing foundation models right\n",
    "out of the box using techniques called prompt engineering and in\u0002context learning.\n",
    "Most commonly, you will start from an existing foundation model (as\n",
    "you will see in Chapter 3). This will greatly improve your time-to\u0002market since you will avoid the pretraining step, which is extremely\n",
    "resource intensive and often requires trillions of words, images, videos,\n",
    "or audio clips to get started. Operating at this scale requires a lot of\n",
    "time, patience, and compute—often millions of GPU hours are required\n",
    "when pretraining from scratch.\n",
    "You also want to consider the size of the foundation model you decide\n",
    "to work with as this will impact the hardware—and cost—needed to\n",
    "train and serve your models. While larger models tend to generalize\n",
    "better to more tasks, this is not always the case and depends on the\n",
    "dataset used during training and tuning.\n",
    "We recommend that you try different models for your generative use\n",
    "case and task. Start with an existing, well-documented, relatively small\n",
    "(e.g., 7 billion-parameter) foundation model to iterate quickly and learn\n",
    "the unique ways of interacting with these generative AI models with a\n",
    "relatively small amount of hardware (compared to the larger 175+\n",
    "billion-parameter models).\n",
    "During development, you would typically start with a playground\n",
    "environment within either Amazon SageMaker JumpStart or Amazon\n",
    "Bedrock. This lets you try different prompts and models quickly, as you\n",
    "will see in Chapter 2. Next, you might use a Jupyter notebook or Python\n",
    "script using an integrated development environment (IDE) like Visual\n",
    "Studio Code (VS Code) or Amazon SageMaker Studio notebooks to\n",
    "prepare your custom datasets to use when experimenting with these\n",
    "generative models. Once you are ready to scale your efforts to a larger\n",
    "distributed cluster, you would then migrate to SageMaker distributed\n",
    "training jobs to scale to a larger compute cluster using accelerators like\n",
    "the NVIDIA GPU or AWS Trainium, as you will see in Chapter 4.\n",
    "While you may be able to avoid accelerators initially, you will very\n",
    "likely need to use them for longer-term development and deployment of\n",
    "more complex models. The sooner you learn the unique—and\n",
    "sometimes obscure—aspects of developing with accelerators like\n",
    "NVIDIA GPUs or AWS Trainium chips, the better. Fortunately, a lot of\n",
    "the complexity has been abstracted by the hardware provider through\n",
    "the NVIDIA CUDA library and AWS Neuron SDK, respectively.\n",
    "Adapt, align, and augment.\n",
    "It’s important to adapt generative models to your specific domain, use\n",
    "case, and task. Chapters 5, 6, 7, and 11 are dedicated to fine-tuning your\n",
    "multimodal generative AI models with your custom datasets to meet\n",
    "your business goals.\n",
    "Additionally, as these generative models become more and more\n",
    "humanlike, it is important that they align with human values and\n",
    "preferences—and, in general, behave well. Chapters 7 and 11 explore a\n",
    "technique called reinforcement learning from human feedback (RLHF)\n",
    "to align your multimodal generative models to be more helpful, honest,\n",
    "and harmless (HHH). RLHF is a key component of the much-broader\n",
    "field of responsible AI.\n",
    "While generative models contain an enormous amount of information\n",
    "and knowledge, they often need to be augmented with current news or\n",
    "proprietary data for your business. In Chapter 9, you will explore ways\n",
    "to augment your generative models with external data sources or APIs.\n",
    "Evaluate.\n",
    "To properly implement generative AI applications, you need to iterate\n",
    "heavily. Therefore, it’s important to establish well-defined evaluation\n",
    "metrics and benchmarks to help measure the effectiveness of fine\u0002tuning. You will learn about model evaluation in Chapter 5. While not\n",
    "as straightforward as traditional machine learning, model evaluation\n",
    "helps measure improvements to your models during the adaptation and\n",
    "alignment phase—specifically, how well the model aligns to your\n",
    "business goals and human preferences.\n",
    "Deploy and integrate.\n",
    "When you finally have a well-tuned and aligned generative model, it’s\n",
    "time to deploy your model for inference and integrate the model into\n",
    "your application. In Chapter 8, you will see how to optimize the model\n",
    "for inference and better utilize your compute resources, reduce\n",
    "inference latency, and delight your users.\n",
    "You will also see how to deploy your models with the AWS Inferentia\n",
    "family of compute instances optimized for generative inference using\n",
    "Amazon SageMaker endpoints. SageMaker endpoints are a great option\n",
    "for serving generative models as they are highly scalable, fault tolerant,\n",
    "and customizable. They offer flexible deployment and scaling options\n",
    "like A/B testing, shadow deployments, and autoscaling, as you will\n",
    "learn in Chapter 8.\n",
    "Monitor.\n",
    "As with any production system, you should set up proper metrics\n",
    "collection and monitoring systems for all components of your\n",
    "generative AI application. In Chapters 8 and 12, you will learn how to\n",
    "utilize Amazon CloudWatch and CloudTrail to monitor your generative\n",
    "AI applications running on AWS. These services are highly\n",
    "customizable, accessible from the AWS console or AWS software\n",
    "development kit (SDK), and integrated with every AWS service\n",
    "including Amazon Bedrock, a managed service for generative AI, which\n",
    "you will explore in Chapter 12.\n",
    "Generative AI on AWS\n",
    "This section will outline the AWS stack of purpose-built generative AI\n",
    "services and features, as shown in Figure 1-4, as well as discuss some of the\n",
    "benefits of using AWS for generative AI.\n",
    "Figure 1-4. AWS services and features supporting generative AI\n",
    "Model providers include those that are building or pretraining foundation\n",
    "models requiring access to powerful, and cost performant, compute and\n",
    "storage resources. For this, AWS offers a range of frameworks and\n",
    "infrastructure to build foundation models. This includes optimized compute\n",
    "instances for generative AI with self-managed options such as Amazon EC2\n",
    "as well as managed options like Amazon SageMaker for model training and\n",
    "model deployment. In addition, AWS offers its own accelerators optimized\n",
    "for training (AWS Trainium) and deploying generative models (AWS\n",
    "Inferentia).\n",
    "AWS Trainium is an accelerator that is purpose-built for high-performance,\n",
    "low-cost training workloads. Similarly, AWS Inferentia is purpose-built for\n",
    "high-throughput, low-cost inference. The infrastructure options on AWS\n",
    "that are optimized for generative AI are used by model providers but also\n",
    "model tuners.\n",
    "Model tuners include those that are adapting or aligning foundation models\n",
    "to their specific domain, use case, and task. This typically requires access to\n",
    "not only storage and compute resources but also tooling that helps enable\n",
    "these tasks through easy access to a range of foundation models while\n",
    "removing the need to manage underlying infrastructure. In addition to the\n",
    "range of optimized infrastructure available on AWS, tuners also have access\n",
    "to a broad range of popular foundation models as well as tooling to adapt or\n",
    "align foundation models, including capabilities built into Amazon Bedrock\n",
    "and Amazon SageMaker JumpStart.\n",
    "Amazon Bedrock is a fully managed service that provides access to models\n",
    "from Amazon (e.g., Titan) and popular third-party providers (e.g., AI21\n",
    "Labs, Anthropic, Cohere, and Stability AI). This allows you to quickly get\n",
    "started experimenting with available foundation models. Bedrock also\n",
    "allows you to privately customize foundation models with your own data as\n",
    "well as integrate and deploy those models into generative AI applications.\n",
    "Agents for Bedrock are fully managed and allow for additional\n",
    "customization with the integration of proprietary external data sources and\n",
    "the ability to complete tasks.\n",
    "Amazon SageMaker JumpStart provides access to both public and\n",
    "proprietary foundation models through a model hub that includes the ability\n",
    "to easily deploy a foundation model to Amazon SageMaker model\n",
    "deployment real-time endpoints. Additionally, SageMaker JumpStart\n",
    "provides the ability to fine-tune available models utilizing SageMaker\n",
    "model training. SageMaker JumpStart automatically generates notebooks\n",
    "with code for deploying and fine-tuning models available on the model hub.\n",
    "Amazon SageMaker provides additional extensibility, through managed\n",
    "environments in Amazon SageMaker Studio notebooks, to work with any\n",
    "available foundation model, regardless of whether it’s available in\n",
    "SageMaker JumpStart. As a result, you have the ability to work with any\n",
    "models accessible to you and are never limited in the models you can work\n",
    "with in Amazon SageMaker.\n",
    "Adapting a model to a specific use case, task, or domain often includes\n",
    "augmenting the model with additional data. AWS also provides multiple\n",
    "implementation options for vector stores that store vector embeddings.\n",
    "Vector stores and embeddings are used for retrieval-augmented generation\n",
    "(RAG) to efficiently retrieve relevant information from external data\n",
    "sources to augment the data used with a generative model.\n",
    "The options available include vector engine for Amazon OpenSearch\n",
    "Serverless as well as the k-NN plugin available for use with Amazon\n",
    "OpenSearch Service. In addition, both Amazon Aurora PostgreSQL and\n",
    "Amazon Relational Database Services (RDS) for PostgreSQL include\n",
    "vector stores capabilities through built-in pgvector support.\n",
    "If you are looking for a fully managed semantic search experience on\n",
    "domain-specific data, you can use Amazon Kendra, which creates and\n",
    "manages the embeddings for you.\n",
    "AWS offers multiple options if you want to access generative models\n",
    "through end-to-end generative AI applications. On AWS, you can build\n",
    "your own custom generative AI applications using the breadth and depth of\n",
    "services available; you can also take advantage of packaged, fully managed,\n",
    "services.\n",
    "For example, Amazon CodeWhisperer provides generative coding\n",
    "capabilities across multiple coding languages, supporting productivity\n",
    "enhancements such as code generation, proactively scanning for\n",
    "vulnerabilities and suggesting code remediations, with automatic\n",
    "suggestions for code attribution.\n",
    "AWS HealthScribe is another packaged generative AI service targeted\n",
    "toward the healthcare industry to allow for the automatic generation of\n",
    "clinical notes based on patient-clinician conversations.\n",
    "Finally, Amazon QuickSight Q includes built-in generative capabilities\n",
    "allowing users to ask questions about data in natural language and receive\n",
    "answers as well as generated visualizations that allow users to gain more\n",
    "insights into their data.\n",
    "This book will largely focus on the personas and tasks involved in the\n",
    "section “Generative AI Project Life Cycle”—as well as building generative\n",
    "AI applications. Many of the services highlighted in this section, such as\n",
    "Amazon SageMaker JumpStart and Amazon Bedrock, will be referenced\n",
    "throughout this book as you dive into specific areas of the generative AI\n",
    "project life cycle.\n",
    "Now that we’ve introduced some core AWS services for generative AI, let’s\n",
    "look at some of the benefits of using AWS to build generative AI\n",
    "applications.\n",
    "Why Generative AI on AWS?\n",
    "Key benefits of utilizing AWS for your generative AI workloads include\n",
    "increased flexibility and choice, enterprise-grade security and governance\n",
    "capabilities, state-of-the art generative AI capabilities, low operational\n",
    "overhead through fully managed services, the ability to quickly get started\n",
    "with ready-to-use solutions and services, and a strong history of continuous\n",
    "innovation. Let’s dive a bit further into each of these with some specific\n",
    "examples:\n",
    "Increased flexibility and choice\n",
    "AWS provides flexibility not only in the ability to utilize a range of\n",
    "services and features to meet the needs of each use case, but also in\n",
    "terms of choice in generative models. This provides you with the ability\n",
    "to not only choose the right model for a use case, but to also change and\n",
    "continually evaluate new models to take advantage of new capabilities.\n",
    "Enterprise-grade security and governance capabilities\n",
    "AWS services are built with security and governance capabilities that\n",
    "are important to the most regulated industries. For example, SageMaker\n",
    "model training, SageMaker model deployment, and Amazon Bedrock\n",
    "support key capabilities around data protection, network isolation,\n",
    "controlled access and authorization, as well as threat detection.\n",
    "State-of-the-art generative AI capabilities\n",
    "AWS offers choice in generative AI models, from Amazon models as\n",
    "well as third-party provider models in Amazon Bedrock to open source\n",
    "and proprietary models offered through Amazon SageMaker JumpStart.\n",
    "Additionally, AWS has also invested in infrastructure like AWS\n",
    "Trainium and AWS Inferentia for training and deploying generative\n",
    "models at scale.\n",
    "Low operational overhead\n",
    "As previously discussed, many of the AWS services and features\n",
    "targeted toward generative AI are offered through managed\n",
    "infrastructure, serverless offerings, or packaged solutions. This allows\n",
    "you to focus on generative AI models and applications instead of\n",
    "managing infrastructure and to quickly get started with ready-to-use\n",
    "solutions and services.\n",
    "Strong history of continuous innovation\n",
    "AWS has an established history of rapid innovation built on years of\n",
    "experience in not only cloud infrastructure but artificial intelligence.\n",
    "The AWS stack of services and features for supporting generative AI covers\n",
    "the breadth, depth, and extensibility to support every use case, whether\n",
    "you’re a model provider, a tuner, or a consumer. In addition to the\n",
    "generative AI capabilities on AWS, a broader set of AWS services also\n",
    "supports the ability to build custom generative AI applications, which will\n",
    "be covered in the next section.\n",
    "Building Generative AI Applications on AWS\n",
    "A generative AI application includes more than generative models. It\n",
    "requires multiple components to build reliable, scalable, and secure\n",
    "applications that are then offered to consumers of that application, whether\n",
    "they are end users or other systems, as shown in Figure 1-5.\n",
    "Figure 1-5. Generative AI applications include more than foundation models\n",
    "When using a packaged generative AI service such as Amazon\n",
    "CodeWhisperer, all of this is completely abstracted and provided to the end\n",
    "user. However, building custom generative AI applications typically\n",
    "requires a range of services. AWS provides the breadth of services that are\n",
    "often required to build an end-to-end generative AI application. Figure 1-6\n",
    "shows an example of AWS services that may be used as part of a broader\n",
    "generative AI application.\n",
    "Figure 1-6. AWS breadth of service to enable customers to build generative AI\n",
    "applications\n",
    "Summary\n",
    "In this chapter, you explored some common generative AI use cases and\n",
    "learned some generative AI fundamentals. You also saw an example of a\n",
    "typical generative AI project life cycle that includes various stages,\n",
    "including defining a use case, prompt engineering (Chapter 2), selecting a\n",
    "foundation model (Chapter 3), fine-tuning (Chapters 5 and 6), aligning with\n",
    "human values (Chapter 7), deploying your model (Chapter 8), and\n",
    "integrating with external data sources and agents (Chapter 9).\n",
    "The compute-intensive parts of the life cycle—including fine-tuning and\n",
    "human alignment—will benefit from an understanding of quantization and\n",
    "distributed-computing algorithms (Chapter 4). These optimizations and\n",
    "algorithms will speed up the iterative development cycle that is critical\n",
    "when developing generative AI models.\n",
    "In Chapter 2, you will learn some prompt engineering tips and best\n",
    "practices. These are useful for prompting both language-only foundation\n",
    "models (Chapter 3) and multimodal foundation models (Chapters 10 and\n",
    "11) using either Amazon SageMaker JumpStart model hub (Chapter 3) or\n",
    "the Amazon Bedrock managed generative AI service (Chapter 12)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "message = questions_manager.add_message_to_thread(\n",
    "    role=\"user\",\n",
    "    content=user_prompt,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Run the Assistant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_assistant = questions_manager.run_assistant(instructions=GENERATE_QUESTIONS_ASSISTANT_SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Check the Run status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_assistant = questions_manager.wait_for_completion(run=run_assistant, thread=thread)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SyncCursorPage[ThreadMessage](data=[ThreadMessage(id='msg_IdTKRL9vmmCEPAQaQi6sZRM9', assistant_id='asst_35nm0GKkIzy6lghgMWZfBHgB', content=[MessageContentText(text=Text(annotations=[], value='Based on the provided content about Applications of Generative AI for Businesses, I have generated 10 quiz questions that cover fundamental concepts about Generative AI. The questions are of three types: single_select_mcq, multi_select_mcq, and open_text_question. Here are the quiz questions:\\n\\n1. **Question 1 (single_select_mcq):**\\n   - **Time Limit:** 1 minute\\n   - **Difficulty:** Easy\\n   - **Question:** What is one of the common use cases of generative AI mentioned in the provided content?\\n   - **Options:**\\n     1. Text summarization\\n     2. Image classification\\n     3. Sentiment analysis\\n     4. Speech recognition\\n\\n2. **Question 2 (multi_select_mcq):**\\n   - **Time Limit:** 1 minute\\n   - **Difficulty:** Easy\\n   - **Question:** Select the tasks associated with generative AI use cases from the following options:\\n   - **Options:**\\n     1. Text summarization\\n     2. Image generation\\n     3. Classification and content moderation\\n     4. Conversational interface\\n\\n3. **Question 3 (open_text_question):**\\n   - **Time Limit:** 1 minute\\n   - **Difficulty:** Easy\\n   - **Question:** Explain the concept of \"Question answering (QA) and visual question answering (VQA)\" as mentioned in the content.\\n\\n4. **Question 4 (single_select_mcq):**\\n   - **Time Limit:** 1 minute\\n   - **Difficulty:** Easy\\n   - **Question:** In generative AI, what task involves handling multiturn conversations through a chat-like interface?\\n   - **Options:**\\n     1. Classification and content moderation\\n     2. Conversational interface\\n     3. Text summarization\\n     4. Source code generation\\n\\n5. **Question 5 (multi_select_mcq):**\\n   - **Time Limit:** 1 minute\\n   - **Difficulty:** Easy\\n   - **Question:** Select the examples of generative AI use cases mentioned in the content:\\n   - **Options:**\\n     1. Producing personalized marketing\\n     2. Editing software code\\n     3. Detecting harmful content\\n     4. Summarizing news articles\\n\\n6. **Question 6 (open_text_question):**\\n   - **Time Limit:** 1 minute\\n   - **Difficulty:** Easy\\n   - **Question:** Describe the importance of fine-tuning in generative AI projects based on the provided content.\\n\\n7. **Question 7 (single_select_mcq):**\\n   - **Time Limit:** 1 minute\\n   - **Difficulty:** Easy\\n   - **Question:** What does generative AI help in extracting from documents, such as names, addresses, events, or numeric data?\\n   - **Options:**\\n     1. Toxic content\\n     2. Personally identifiable information\\n     3. Visual information\\n     4. Audio clips\\n\\n8. **Question 8 (multi_select_mcq):**\\n   - **Time Limit:** 1 minute\\n   - **Difficulty:** Easy\\n   - **Question:** Which of the following tasks are associated with an extension to the question-answer task in generative AI?\\n   - **Options:**\\n     1. Classification and content moderation\\n     2. Masking personally identifiable information\\n     3. Detecting toxic or harmful content\\n     4. Reasoning through problems\\n\\n9. **Question 9 (open_text_question):**\\n   - **Time Limit:** 1 minute\\n   - **Difficulty:** Easy\\n   - **Question:** Discuss the significance of adapting generative models to specific domains, as mentioned in the content.\\n\\n10. **Question 10 (single_select_mcq):**\\n   - **Time Limit:** 1 minute\\n   - **Difficulty:** Easy\\n   - **Question:** What does Amazon SageMaker JumpStart provide access to in the generative AI context?\\n   - **Options:**\\n     1. Foundation models\\n     2. Hardware accelerators\\n     3. Enterprise software solutions\\n     4. Data storage services\\n\\nThese quiz questions aim to test understanding and knowledge about the applications and fundamentals of Generative AI for businesses.'), type='text')], created_at=1708701463, file_ids=[], metadata={}, object='thread.message', role='assistant', run_id='run_yELO1bOVCZQVfz3gPwIzyJDL', thread_id='thread_OwC2Nx9eHHZtv15YUdxSPppv'), ThreadMessage(id='msg_Z5CAQuYTW8ewa0oDIJ3hxEs9', assistant_id=None, content=[MessageContentText(text=Text(annotations=[], value='Create a 30 minutes \"Generative AI\" that will have 10 Questions. The Questions type will include: 1. \"single_select_mcq\" 2. \"multi_select_mcq\" and \"open_text_question\". The Quiz will be easy and the questions generated shall follow: \"Create a quiz about Applications of Generative AI for Businesses. It will conver fundamental concepts about it. The Content from which Quiz Shall be generated is: \"Chapter 1. Generative AI Use Cases,\\nFundamentals, and Project Life Cycle\\nIn this chapter, you will see some generative AI tasks and use cases in\\naction, gain an understanding of generative foundation models, and explore\\na typical generative AI project life cycle. The use cases and tasks you’ll see\\nin this chapter include intelligent search, automated customer-support\\nchatbot, dialog summarization, not-safe-for-work (NSFW) content\\nmoderation, personalized product videos, source code generation, and\\nothers.\\nYou will also learn a few of the generative AI service and hardware options\\nfrom Amazon Web Services (AWS) including Amazon Bedrock, Amazon\\nSageMaker, Amazon CodeWhisperer, AWS Trainium, and AWS Inferentia.\\nThese service and hardware options provide great flexibility when building\\nyour end-to-end, context-aware, multimodal reasoning applications with\\ngenerative AI on AWS.\\nLet’s explore some common use cases and tasks for generative AI.\\nUse Cases and Tasks\\nSimilar to deep learning, generative AI is a general-purpose technology\\nused for multiple purposes across many industries and customer segments.\\nThere are many types of multimodal generative AI tasks. We’ve included a\\nlist of the most common generative tasks and associated example use cases:\\nText summarization\\nProduce a shorter version of a piece of text while retaining the main\\nideas. Examples include summarizing a news article, legal document, or\\nfinancial report into a smaller number of words or paragraphs for faster\\nconsumption. Often, summarization is used on customer support\\nconversations to provide a quick overview of the interaction between a\\ncustomer and support representative.\\nRewriting\\nModify the wording of a piece of text to adapt to a different audience,\\nformality, or tone. For example, you can convert a formal legal\\ndocument into a less formal document using less legal terms to appeal\\nto a nonlegal audience.\\nInformation extraction\\nExtract information from documents such as names, addresses, events,\\nor numeric data or numbers. For example, converting an email into a\\npurchase order in an enterprise resource planning (ERP) system like\\nSAP.\\nQuestion answering (QA) and visual question answering (VQA)\\nAsk questions directly against a set of documents, images, videos, or\\naudio clips. For example, you can set up an internal, employee-facing\\nchatbot to answer questions about human resources and benefits\\ndocuments.\\nDetecting toxic or harmful content\\nAn extension to the question-answer task, you can ask a generative\\nmodel if a set of text, images, videos, or audio clips contains any\\ntoxicity or harmful content.\\nClassification and content moderation\\nAssign a category to a given piece of content such as a document,\\nimage, video, or audio clip. For example, deleting email spam, filtering\\nout inappropriate images, or labeling incoming, text-based customer\\x02support tickets.\\nConversational interface\\nHandle multiturn conversations to accomplish tasks through a chat-like\\ninterface. Examples include chatbots for self-service customer support\\nor mental health therapy sessions.\\nTranslation\\nOne of the earliest use cases for generative AI is language translation.\\nConsider, for example, that the publisher of this book wants to release a\\nGerman translation to help expand the book’s reach. Or perhaps you\\nmay want to convert the Python-based examples to Java to work within\\nyour existing Java-based enterprise application.\\nSource code generation\\nCreate source code from natural language code comments—or even a\\nhand-drawn sketch, as shown in Figure 1-1. Here, an HTML- and\\nJavaScript-based website is generated from a UI sketch scribbled on the\\nback of a restaurant napkin.\\nFigure 1-1. Generating UI code from hand-drawn sketch\\nReasoning\\nReason through a problem to discover potential new solutions, trade\\x02offs, or latent details. For example, consider a CFO who provides an\\naudio-based quarterly financial report to investors as well as a more\\x02detailed written report. By reasoning through these different media\\nformats together, the model may discover some conclusions about the\\ncompany’s health not directly mentioned in the audio or stated in the\\ntext.\\nMask personally identifiable information (PII)\\nYou can use generative models to mask personally identifiable\\ninformation from a given corpus of text. This is useful for many use\\ncases where you are working with sensitive data and wish to remove PII\\ndata from your workflows.\\nPersonalized marketing and ads\\nGenerate personalized product descriptions, videos, or ads based on user\\nprofile features. Consider an ecommerce website that wants to generate\\na personalized description for each product based on the logged-in\\nuser’s age or family situation. You could also generate personalized\\nproduct images that include mature adults, adults with children, or\\nchildren themselves to better appeal to the logged-in user’s\\ndemographic, as shown in Figure 1-2.\\nFigure 1-2. Personalized marketing\\nIn this case, each user of the service would potentially see a unique and\\nhighly personalized image and description for the same product. This\\ncould ultimately lead to more product clicks and higher sales.\\nIn each of these generative use cases and tasks, a model creates content that\\napproximates a human’s understanding of language. This is truly amazing\\nand is made possible by a neural network architecture called the\\ntransformer, which you will learn in Chapter 3.\\nIn the next section, you will learn how to access foundation models through\\nmodel hubs.\\nFoundation Models and Model Hubs\\nFoundation models are very large and complex neural network models\\nconsisting of billions of parameters (a.k.a. weights). The model parameters\\nare learned during the training phase—often called pretraining. Foundation\\nmodels are trained on massive amounts of training data—typically over a\\nperiod of many weeks and months using large, distributed clusters of CPUs\\nand graphics processing units (GPUs). After learning billions of parameters,\\nthese foundation models can represent complex entities such as human\\nlanguage, images, videos, and audio clips.\\nIn most cases, you will start your generative AI projects with an existing\\nfoundation model from a model hub such as Hugging Face Model Hub,\\nPyTorch Hub, or Amazon SageMaker JumpStart. A model hub is a\\ncollection of models that typically contains detailed model descriptions\\nincluding the use cases that they address.\\nThroughout this book, we will use Hugging Face Model Hub and\\nSageMaker JumpStart to access foundation models like Llama 2 from Meta\\n(Facebook) and Falcon from the Technology Innovation Institute (TII) and\\nFLAN-T5 from Google. You will dive deeper into model hubs and\\nfoundation models in Chapter 3.\\nNext, you’ll see a typical generative AI project life cycle that roughly\\nfollows the outline of the rest of this book.\\nGenerative AI Project Life Cycle\\nWhile there is no definitive project life cycle for generative AI projects, the\\nframework shown in Figure 1-3 can help guide you through the most\\nimportant parts of your generative AI application journey. Throughout the\\nbook, you will gain intuition, learn to avoid potential difficulties, and\\nimprove your decision making at each step in the journey.\\nFigure 1-3. Generative AI project life cycle framework\\nLet’s dive into each component of the life cycle shown in Figure 1-3:\\nIdentify use case.\\nAs with any project, you first want to define your scope, including the\\nspecific generative use case and task that you plan to address with your\\ngenerative AI application. We recommend that you start with a single,\\nwell-documented generative use case. This will help you get familiar\\nwith the environment and understand the power—and limitations—of\\nthese models without trying to optimize the model for different tasks at\\nthe same time. While these models are capable of carrying out multiple\\ntasks, it’s a bit more difficult to evaluate and optimize the model across\\nmultiple tasks to start.\\nExperiment and select.\\nGenerative AI models are capable of carrying out many different tasks\\nwith great success. However, you will need to decide if an existing\\nfoundation model is suitable for your application needs. In Chapter 2,\\nyou will learn how to work with these existing foundation models right\\nout of the box using techniques called prompt engineering and in\\x02context learning.\\nMost commonly, you will start from an existing foundation model (as\\nyou will see in Chapter 3). This will greatly improve your time-to\\x02market since you will avoid the pretraining step, which is extremely\\nresource intensive and often requires trillions of words, images, videos,\\nor audio clips to get started. Operating at this scale requires a lot of\\ntime, patience, and compute—often millions of GPU hours are required\\nwhen pretraining from scratch.\\nYou also want to consider the size of the foundation model you decide\\nto work with as this will impact the hardware—and cost—needed to\\ntrain and serve your models. While larger models tend to generalize\\nbetter to more tasks, this is not always the case and depends on the\\ndataset used during training and tuning.\\nWe recommend that you try different models for your generative use\\ncase and task. Start with an existing, well-documented, relatively small\\n(e.g., 7 billion-parameter) foundation model to iterate quickly and learn\\nthe unique ways of interacting with these generative AI models with a\\nrelatively small amount of hardware (compared to the larger 175+\\nbillion-parameter models).\\nDuring development, you would typically start with a playground\\nenvironment within either Amazon SageMaker JumpStart or Amazon\\nBedrock. This lets you try different prompts and models quickly, as you\\nwill see in Chapter 2. Next, you might use a Jupyter notebook or Python\\nscript using an integrated development environment (IDE) like Visual\\nStudio Code (VS Code) or Amazon SageMaker Studio notebooks to\\nprepare your custom datasets to use when experimenting with these\\ngenerative models. Once you are ready to scale your efforts to a larger\\ndistributed cluster, you would then migrate to SageMaker distributed\\ntraining jobs to scale to a larger compute cluster using accelerators like\\nthe NVIDIA GPU or AWS Trainium, as you will see in Chapter 4.\\nWhile you may be able to avoid accelerators initially, you will very\\nlikely need to use them for longer-term development and deployment of\\nmore complex models. The sooner you learn the unique—and\\nsometimes obscure—aspects of developing with accelerators like\\nNVIDIA GPUs or AWS Trainium chips, the better. Fortunately, a lot of\\nthe complexity has been abstracted by the hardware provider through\\nthe NVIDIA CUDA library and AWS Neuron SDK, respectively.\\nAdapt, align, and augment.\\nIt’s important to adapt generative models to your specific domain, use\\ncase, and task. Chapters 5, 6, 7, and 11 are dedicated to fine-tuning your\\nmultimodal generative AI models with your custom datasets to meet\\nyour business goals.\\nAdditionally, as these generative models become more and more\\nhumanlike, it is important that they align with human values and\\npreferences—and, in general, behave well. Chapters 7 and 11 explore a\\ntechnique called reinforcement learning from human feedback (RLHF)\\nto align your multimodal generative models to be more helpful, honest,\\nand harmless (HHH). RLHF is a key component of the much-broader\\nfield of responsible AI.\\nWhile generative models contain an enormous amount of information\\nand knowledge, they often need to be augmented with current news or\\nproprietary data for your business. In Chapter 9, you will explore ways\\nto augment your generative models with external data sources or APIs.\\nEvaluate.\\nTo properly implement generative AI applications, you need to iterate\\nheavily. Therefore, it’s important to establish well-defined evaluation\\nmetrics and benchmarks to help measure the effectiveness of fine\\x02tuning. You will learn about model evaluation in Chapter 5. While not\\nas straightforward as traditional machine learning, model evaluation\\nhelps measure improvements to your models during the adaptation and\\nalignment phase—specifically, how well the model aligns to your\\nbusiness goals and human preferences.\\nDeploy and integrate.\\nWhen you finally have a well-tuned and aligned generative model, it’s\\ntime to deploy your model for inference and integrate the model into\\nyour application. In Chapter 8, you will see how to optimize the model\\nfor inference and better utilize your compute resources, reduce\\ninference latency, and delight your users.\\nYou will also see how to deploy your models with the AWS Inferentia\\nfamily of compute instances optimized for generative inference using\\nAmazon SageMaker endpoints. SageMaker endpoints are a great option\\nfor serving generative models as they are highly scalable, fault tolerant,\\nand customizable. They offer flexible deployment and scaling options\\nlike A/B testing, shadow deployments, and autoscaling, as you will\\nlearn in Chapter 8.\\nMonitor.\\nAs with any production system, you should set up proper metrics\\ncollection and monitoring systems for all components of your\\ngenerative AI application. In Chapters 8 and 12, you will learn how to\\nutilize Amazon CloudWatch and CloudTrail to monitor your generative\\nAI applications running on AWS. These services are highly\\ncustomizable, accessible from the AWS console or AWS software\\ndevelopment kit (SDK), and integrated with every AWS service\\nincluding Amazon Bedrock, a managed service for generative AI, which\\nyou will explore in Chapter 12.\\nGenerative AI on AWS\\nThis section will outline the AWS stack of purpose-built generative AI\\nservices and features, as shown in Figure 1-4, as well as discuss some of the\\nbenefits of using AWS for generative AI.\\nFigure 1-4. AWS services and features supporting generative AI\\nModel providers include those that are building or pretraining foundation\\nmodels requiring access to powerful, and cost performant, compute and\\nstorage resources. For this, AWS offers a range of frameworks and\\ninfrastructure to build foundation models. This includes optimized compute\\ninstances for generative AI with self-managed options such as Amazon EC2\\nas well as managed options like Amazon SageMaker for model training and\\nmodel deployment. In addition, AWS offers its own accelerators optimized\\nfor training (AWS Trainium) and deploying generative models (AWS\\nInferentia).\\nAWS Trainium is an accelerator that is purpose-built for high-performance,\\nlow-cost training workloads. Similarly, AWS Inferentia is purpose-built for\\nhigh-throughput, low-cost inference. The infrastructure options on AWS\\nthat are optimized for generative AI are used by model providers but also\\nmodel tuners.\\nModel tuners include those that are adapting or aligning foundation models\\nto their specific domain, use case, and task. This typically requires access to\\nnot only storage and compute resources but also tooling that helps enable\\nthese tasks through easy access to a range of foundation models while\\nremoving the need to manage underlying infrastructure. In addition to the\\nrange of optimized infrastructure available on AWS, tuners also have access\\nto a broad range of popular foundation models as well as tooling to adapt or\\nalign foundation models, including capabilities built into Amazon Bedrock\\nand Amazon SageMaker JumpStart.\\nAmazon Bedrock is a fully managed service that provides access to models\\nfrom Amazon (e.g., Titan) and popular third-party providers (e.g., AI21\\nLabs, Anthropic, Cohere, and Stability AI). This allows you to quickly get\\nstarted experimenting with available foundation models. Bedrock also\\nallows you to privately customize foundation models with your own data as\\nwell as integrate and deploy those models into generative AI applications.\\nAgents for Bedrock are fully managed and allow for additional\\ncustomization with the integration of proprietary external data sources and\\nthe ability to complete tasks.\\nAmazon SageMaker JumpStart provides access to both public and\\nproprietary foundation models through a model hub that includes the ability\\nto easily deploy a foundation model to Amazon SageMaker model\\ndeployment real-time endpoints. Additionally, SageMaker JumpStart\\nprovides the ability to fine-tune available models utilizing SageMaker\\nmodel training. SageMaker JumpStart automatically generates notebooks\\nwith code for deploying and fine-tuning models available on the model hub.\\nAmazon SageMaker provides additional extensibility, through managed\\nenvironments in Amazon SageMaker Studio notebooks, to work with any\\navailable foundation model, regardless of whether it’s available in\\nSageMaker JumpStart. As a result, you have the ability to work with any\\nmodels accessible to you and are never limited in the models you can work\\nwith in Amazon SageMaker.\\nAdapting a model to a specific use case, task, or domain often includes\\naugmenting the model with additional data. AWS also provides multiple\\nimplementation options for vector stores that store vector embeddings.\\nVector stores and embeddings are used for retrieval-augmented generation\\n(RAG) to efficiently retrieve relevant information from external data\\nsources to augment the data used with a generative model.\\nThe options available include vector engine for Amazon OpenSearch\\nServerless as well as the k-NN plugin available for use with Amazon\\nOpenSearch Service. In addition, both Amazon Aurora PostgreSQL and\\nAmazon Relational Database Services (RDS) for PostgreSQL include\\nvector stores capabilities through built-in pgvector support.\\nIf you are looking for a fully managed semantic search experience on\\ndomain-specific data, you can use Amazon Kendra, which creates and\\nmanages the embeddings for you.\\nAWS offers multiple options if you want to access generative models\\nthrough end-to-end generative AI applications. On AWS, you can build\\nyour own custom generative AI applications using the breadth and depth of\\nservices available; you can also take advantage of packaged, fully managed,\\nservices.\\nFor example, Amazon CodeWhisperer provides generative coding\\ncapabilities across multiple coding languages, supporting productivity\\nenhancements such as code generation, proactively scanning for\\nvulnerabilities and suggesting code remediations, with automatic\\nsuggestions for code attribution.\\nAWS HealthScribe is another packaged generative AI service targeted\\ntoward the healthcare industry to allow for the automatic generation of\\nclinical notes based on patient-clinician conversations.\\nFinally, Amazon QuickSight Q includes built-in generative capabilities\\nallowing users to ask questions about data in natural language and receive\\nanswers as well as generated visualizations that allow users to gain more\\ninsights into their data.\\nThis book will largely focus on the personas and tasks involved in the\\nsection “Generative AI Project Life Cycle”—as well as building generative\\nAI applications. Many of the services highlighted in this section, such as\\nAmazon SageMaker JumpStart and Amazon Bedrock, will be referenced\\nthroughout this book as you dive into specific areas of the generative AI\\nproject life cycle.\\nNow that we’ve introduced some core AWS services for generative AI, let’s\\nlook at some of the benefits of using AWS to build generative AI\\napplications.\\nWhy Generative AI on AWS?\\nKey benefits of utilizing AWS for your generative AI workloads include\\nincreased flexibility and choice, enterprise-grade security and governance\\ncapabilities, state-of-the art generative AI capabilities, low operational\\noverhead through fully managed services, the ability to quickly get started\\nwith ready-to-use solutions and services, and a strong history of continuous\\ninnovation. Let’s dive a bit further into each of these with some specific\\nexamples:\\nIncreased flexibility and choice\\nAWS provides flexibility not only in the ability to utilize a range of\\nservices and features to meet the needs of each use case, but also in\\nterms of choice in generative models. This provides you with the ability\\nto not only choose the right model for a use case, but to also change and\\ncontinually evaluate new models to take advantage of new capabilities.\\nEnterprise-grade security and governance capabilities\\nAWS services are built with security and governance capabilities that\\nare important to the most regulated industries. For example, SageMaker\\nmodel training, SageMaker model deployment, and Amazon Bedrock\\nsupport key capabilities around data protection, network isolation,\\ncontrolled access and authorization, as well as threat detection.\\nState-of-the-art generative AI capabilities\\nAWS offers choice in generative AI models, from Amazon models as\\nwell as third-party provider models in Amazon Bedrock to open source\\nand proprietary models offered through Amazon SageMaker JumpStart.\\nAdditionally, AWS has also invested in infrastructure like AWS\\nTrainium and AWS Inferentia for training and deploying generative\\nmodels at scale.\\nLow operational overhead\\nAs previously discussed, many of the AWS services and features\\ntargeted toward generative AI are offered through managed\\ninfrastructure, serverless offerings, or packaged solutions. This allows\\nyou to focus on generative AI models and applications instead of\\nmanaging infrastructure and to quickly get started with ready-to-use\\nsolutions and services.\\nStrong history of continuous innovation\\nAWS has an established history of rapid innovation built on years of\\nexperience in not only cloud infrastructure but artificial intelligence.\\nThe AWS stack of services and features for supporting generative AI covers\\nthe breadth, depth, and extensibility to support every use case, whether\\nyou’re a model provider, a tuner, or a consumer. In addition to the\\ngenerative AI capabilities on AWS, a broader set of AWS services also\\nsupports the ability to build custom generative AI applications, which will\\nbe covered in the next section.\\nBuilding Generative AI Applications on AWS\\nA generative AI application includes more than generative models. It\\nrequires multiple components to build reliable, scalable, and secure\\napplications that are then offered to consumers of that application, whether\\nthey are end users or other systems, as shown in Figure 1-5.\\nFigure 1-5. Generative AI applications include more than foundation models\\nWhen using a packaged generative AI service such as Amazon\\nCodeWhisperer, all of this is completely abstracted and provided to the end\\nuser. However, building custom generative AI applications typically\\nrequires a range of services. AWS provides the breadth of services that are\\noften required to build an end-to-end generative AI application. Figure 1-6\\nshows an example of AWS services that may be used as part of a broader\\ngenerative AI application.\\nFigure 1-6. AWS breadth of service to enable customers to build generative AI\\napplications\\nSummary\\nIn this chapter, you explored some common generative AI use cases and\\nlearned some generative AI fundamentals. You also saw an example of a\\ntypical generative AI project life cycle that includes various stages,\\nincluding defining a use case, prompt engineering (Chapter 2), selecting a\\nfoundation model (Chapter 3), fine-tuning (Chapters 5 and 6), aligning with\\nhuman values (Chapter 7), deploying your model (Chapter 8), and\\nintegrating with external data sources and agents (Chapter 9).\\nThe compute-intensive parts of the life cycle—including fine-tuning and\\nhuman alignment—will benefit from an understanding of quantization and\\ndistributed-computing algorithms (Chapter 4). These optimizations and\\nalgorithms will speed up the iterative development cycle that is critical\\nwhen developing generative AI models.\\nIn Chapter 2, you will learn some prompt engineering tips and best\\npractices. These are useful for prompting both language-only foundation\\nmodels (Chapter 3) and multimodal foundation models (Chapters 10 and\\n11) using either Amazon SageMaker JumpStart model hub (Chapter 3) or\\nthe Amazon Bedrock managed generative AI service (Chapter 12)'), type='text')], created_at=1708701150, file_ids=[], metadata={}, object='thread.message', role='user', run_id=None, thread_id='thread_OwC2Nx9eHHZtv15YUdxSPppv')], object='list', first_id='msg_IdTKRL9vmmCEPAQaQi6sZRM9', last_id='msg_Z5CAQuYTW8ewa0oDIJ3hxEs9', has_more=False)\n"
     ]
    }
   ],
   "source": [
    "print(check_assistant)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_last_message = questions_manager.process_messages()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SyncCursorPage[ThreadMessage](data=[ThreadMessage(id='msg_IdTKRL9vmmCEPAQaQi6sZRM9', assistant_id='asst_35nm0GKkIzy6lghgMWZfBHgB', content=[MessageContentText(text=Text(annotations=[], value='Based on the provided content about Applications of Generative AI for Businesses, I have generated 10 quiz questions that cover fundamental concepts about Generative AI. The questions are of three types: single_select_mcq, multi_select_mcq, and open_text_question. Here are the quiz questions:\\n\\n1. **Question 1 (single_select_mcq):**\\n   - **Time Limit:** 1 minute\\n   - **Difficulty:** Easy\\n   - **Question:** What is one of the common use cases of generative AI mentioned in the provided content?\\n   - **Options:**\\n     1. Text summarization\\n     2. Image classification\\n     3. Sentiment analysis\\n     4. Speech recognition\\n\\n2. **Question 2 (multi_select_mcq):**\\n   - **Time Limit:** 1 minute\\n   - **Difficulty:** Easy\\n   - **Question:** Select the tasks associated with generative AI use cases from the following options:\\n   - **Options:**\\n     1. Text summarization\\n     2. Image generation\\n     3. Classification and content moderation\\n     4. Conversational interface\\n\\n3. **Question 3 (open_text_question):**\\n   - **Time Limit:** 1 minute\\n   - **Difficulty:** Easy\\n   - **Question:** Explain the concept of \"Question answering (QA) and visual question answering (VQA)\" as mentioned in the content.\\n\\n4. **Question 4 (single_select_mcq):**\\n   - **Time Limit:** 1 minute\\n   - **Difficulty:** Easy\\n   - **Question:** In generative AI, what task involves handling multiturn conversations through a chat-like interface?\\n   - **Options:**\\n     1. Classification and content moderation\\n     2. Conversational interface\\n     3. Text summarization\\n     4. Source code generation\\n\\n5. **Question 5 (multi_select_mcq):**\\n   - **Time Limit:** 1 minute\\n   - **Difficulty:** Easy\\n   - **Question:** Select the examples of generative AI use cases mentioned in the content:\\n   - **Options:**\\n     1. Producing personalized marketing\\n     2. Editing software code\\n     3. Detecting harmful content\\n     4. Summarizing news articles\\n\\n6. **Question 6 (open_text_question):**\\n   - **Time Limit:** 1 minute\\n   - **Difficulty:** Easy\\n   - **Question:** Describe the importance of fine-tuning in generative AI projects based on the provided content.\\n\\n7. **Question 7 (single_select_mcq):**\\n   - **Time Limit:** 1 minute\\n   - **Difficulty:** Easy\\n   - **Question:** What does generative AI help in extracting from documents, such as names, addresses, events, or numeric data?\\n   - **Options:**\\n     1. Toxic content\\n     2. Personally identifiable information\\n     3. Visual information\\n     4. Audio clips\\n\\n8. **Question 8 (multi_select_mcq):**\\n   - **Time Limit:** 1 minute\\n   - **Difficulty:** Easy\\n   - **Question:** Which of the following tasks are associated with an extension to the question-answer task in generative AI?\\n   - **Options:**\\n     1. Classification and content moderation\\n     2. Masking personally identifiable information\\n     3. Detecting toxic or harmful content\\n     4. Reasoning through problems\\n\\n9. **Question 9 (open_text_question):**\\n   - **Time Limit:** 1 minute\\n   - **Difficulty:** Easy\\n   - **Question:** Discuss the significance of adapting generative models to specific domains, as mentioned in the content.\\n\\n10. **Question 10 (single_select_mcq):**\\n   - **Time Limit:** 1 minute\\n   - **Difficulty:** Easy\\n   - **Question:** What does Amazon SageMaker JumpStart provide access to in the generative AI context?\\n   - **Options:**\\n     1. Foundation models\\n     2. Hardware accelerators\\n     3. Enterprise software solutions\\n     4. Data storage services\\n\\nThese quiz questions aim to test understanding and knowledge about the applications and fundamentals of Generative AI for businesses.'), type='text')], created_at=1708701463, file_ids=[], metadata={}, object='thread.message', role='assistant', run_id='run_yELO1bOVCZQVfz3gPwIzyJDL', thread_id='thread_OwC2Nx9eHHZtv15YUdxSPppv'), ThreadMessage(id='msg_Z5CAQuYTW8ewa0oDIJ3hxEs9', assistant_id=None, content=[MessageContentText(text=Text(annotations=[], value='Create a 30 minutes \"Generative AI\" that will have 10 Questions. The Questions type will include: 1. \"single_select_mcq\" 2. \"multi_select_mcq\" and \"open_text_question\". The Quiz will be easy and the questions generated shall follow: \"Create a quiz about Applications of Generative AI for Businesses. It will conver fundamental concepts about it. The Content from which Quiz Shall be generated is: \"Chapter 1. Generative AI Use Cases,\\nFundamentals, and Project Life Cycle\\nIn this chapter, you will see some generative AI tasks and use cases in\\naction, gain an understanding of generative foundation models, and explore\\na typical generative AI project life cycle. The use cases and tasks you’ll see\\nin this chapter include intelligent search, automated customer-support\\nchatbot, dialog summarization, not-safe-for-work (NSFW) content\\nmoderation, personalized product videos, source code generation, and\\nothers.\\nYou will also learn a few of the generative AI service and hardware options\\nfrom Amazon Web Services (AWS) including Amazon Bedrock, Amazon\\nSageMaker, Amazon CodeWhisperer, AWS Trainium, and AWS Inferentia.\\nThese service and hardware options provide great flexibility when building\\nyour end-to-end, context-aware, multimodal reasoning applications with\\ngenerative AI on AWS.\\nLet’s explore some common use cases and tasks for generative AI.\\nUse Cases and Tasks\\nSimilar to deep learning, generative AI is a general-purpose technology\\nused for multiple purposes across many industries and customer segments.\\nThere are many types of multimodal generative AI tasks. We’ve included a\\nlist of the most common generative tasks and associated example use cases:\\nText summarization\\nProduce a shorter version of a piece of text while retaining the main\\nideas. Examples include summarizing a news article, legal document, or\\nfinancial report into a smaller number of words or paragraphs for faster\\nconsumption. Often, summarization is used on customer support\\nconversations to provide a quick overview of the interaction between a\\ncustomer and support representative.\\nRewriting\\nModify the wording of a piece of text to adapt to a different audience,\\nformality, or tone. For example, you can convert a formal legal\\ndocument into a less formal document using less legal terms to appeal\\nto a nonlegal audience.\\nInformation extraction\\nExtract information from documents such as names, addresses, events,\\nor numeric data or numbers. For example, converting an email into a\\npurchase order in an enterprise resource planning (ERP) system like\\nSAP.\\nQuestion answering (QA) and visual question answering (VQA)\\nAsk questions directly against a set of documents, images, videos, or\\naudio clips. For example, you can set up an internal, employee-facing\\nchatbot to answer questions about human resources and benefits\\ndocuments.\\nDetecting toxic or harmful content\\nAn extension to the question-answer task, you can ask a generative\\nmodel if a set of text, images, videos, or audio clips contains any\\ntoxicity or harmful content.\\nClassification and content moderation\\nAssign a category to a given piece of content such as a document,\\nimage, video, or audio clip. For example, deleting email spam, filtering\\nout inappropriate images, or labeling incoming, text-based customer\\x02support tickets.\\nConversational interface\\nHandle multiturn conversations to accomplish tasks through a chat-like\\ninterface. Examples include chatbots for self-service customer support\\nor mental health therapy sessions.\\nTranslation\\nOne of the earliest use cases for generative AI is language translation.\\nConsider, for example, that the publisher of this book wants to release a\\nGerman translation to help expand the book’s reach. Or perhaps you\\nmay want to convert the Python-based examples to Java to work within\\nyour existing Java-based enterprise application.\\nSource code generation\\nCreate source code from natural language code comments—or even a\\nhand-drawn sketch, as shown in Figure 1-1. Here, an HTML- and\\nJavaScript-based website is generated from a UI sketch scribbled on the\\nback of a restaurant napkin.\\nFigure 1-1. Generating UI code from hand-drawn sketch\\nReasoning\\nReason through a problem to discover potential new solutions, trade\\x02offs, or latent details. For example, consider a CFO who provides an\\naudio-based quarterly financial report to investors as well as a more\\x02detailed written report. By reasoning through these different media\\nformats together, the model may discover some conclusions about the\\ncompany’s health not directly mentioned in the audio or stated in the\\ntext.\\nMask personally identifiable information (PII)\\nYou can use generative models to mask personally identifiable\\ninformation from a given corpus of text. This is useful for many use\\ncases where you are working with sensitive data and wish to remove PII\\ndata from your workflows.\\nPersonalized marketing and ads\\nGenerate personalized product descriptions, videos, or ads based on user\\nprofile features. Consider an ecommerce website that wants to generate\\na personalized description for each product based on the logged-in\\nuser’s age or family situation. You could also generate personalized\\nproduct images that include mature adults, adults with children, or\\nchildren themselves to better appeal to the logged-in user’s\\ndemographic, as shown in Figure 1-2.\\nFigure 1-2. Personalized marketing\\nIn this case, each user of the service would potentially see a unique and\\nhighly personalized image and description for the same product. This\\ncould ultimately lead to more product clicks and higher sales.\\nIn each of these generative use cases and tasks, a model creates content that\\napproximates a human’s understanding of language. This is truly amazing\\nand is made possible by a neural network architecture called the\\ntransformer, which you will learn in Chapter 3.\\nIn the next section, you will learn how to access foundation models through\\nmodel hubs.\\nFoundation Models and Model Hubs\\nFoundation models are very large and complex neural network models\\nconsisting of billions of parameters (a.k.a. weights). The model parameters\\nare learned during the training phase—often called pretraining. Foundation\\nmodels are trained on massive amounts of training data—typically over a\\nperiod of many weeks and months using large, distributed clusters of CPUs\\nand graphics processing units (GPUs). After learning billions of parameters,\\nthese foundation models can represent complex entities such as human\\nlanguage, images, videos, and audio clips.\\nIn most cases, you will start your generative AI projects with an existing\\nfoundation model from a model hub such as Hugging Face Model Hub,\\nPyTorch Hub, or Amazon SageMaker JumpStart. A model hub is a\\ncollection of models that typically contains detailed model descriptions\\nincluding the use cases that they address.\\nThroughout this book, we will use Hugging Face Model Hub and\\nSageMaker JumpStart to access foundation models like Llama 2 from Meta\\n(Facebook) and Falcon from the Technology Innovation Institute (TII) and\\nFLAN-T5 from Google. You will dive deeper into model hubs and\\nfoundation models in Chapter 3.\\nNext, you’ll see a typical generative AI project life cycle that roughly\\nfollows the outline of the rest of this book.\\nGenerative AI Project Life Cycle\\nWhile there is no definitive project life cycle for generative AI projects, the\\nframework shown in Figure 1-3 can help guide you through the most\\nimportant parts of your generative AI application journey. Throughout the\\nbook, you will gain intuition, learn to avoid potential difficulties, and\\nimprove your decision making at each step in the journey.\\nFigure 1-3. Generative AI project life cycle framework\\nLet’s dive into each component of the life cycle shown in Figure 1-3:\\nIdentify use case.\\nAs with any project, you first want to define your scope, including the\\nspecific generative use case and task that you plan to address with your\\ngenerative AI application. We recommend that you start with a single,\\nwell-documented generative use case. This will help you get familiar\\nwith the environment and understand the power—and limitations—of\\nthese models without trying to optimize the model for different tasks at\\nthe same time. While these models are capable of carrying out multiple\\ntasks, it’s a bit more difficult to evaluate and optimize the model across\\nmultiple tasks to start.\\nExperiment and select.\\nGenerative AI models are capable of carrying out many different tasks\\nwith great success. However, you will need to decide if an existing\\nfoundation model is suitable for your application needs. In Chapter 2,\\nyou will learn how to work with these existing foundation models right\\nout of the box using techniques called prompt engineering and in\\x02context learning.\\nMost commonly, you will start from an existing foundation model (as\\nyou will see in Chapter 3). This will greatly improve your time-to\\x02market since you will avoid the pretraining step, which is extremely\\nresource intensive and often requires trillions of words, images, videos,\\nor audio clips to get started. Operating at this scale requires a lot of\\ntime, patience, and compute—often millions of GPU hours are required\\nwhen pretraining from scratch.\\nYou also want to consider the size of the foundation model you decide\\nto work with as this will impact the hardware—and cost—needed to\\ntrain and serve your models. While larger models tend to generalize\\nbetter to more tasks, this is not always the case and depends on the\\ndataset used during training and tuning.\\nWe recommend that you try different models for your generative use\\ncase and task. Start with an existing, well-documented, relatively small\\n(e.g., 7 billion-parameter) foundation model to iterate quickly and learn\\nthe unique ways of interacting with these generative AI models with a\\nrelatively small amount of hardware (compared to the larger 175+\\nbillion-parameter models).\\nDuring development, you would typically start with a playground\\nenvironment within either Amazon SageMaker JumpStart or Amazon\\nBedrock. This lets you try different prompts and models quickly, as you\\nwill see in Chapter 2. Next, you might use a Jupyter notebook or Python\\nscript using an integrated development environment (IDE) like Visual\\nStudio Code (VS Code) or Amazon SageMaker Studio notebooks to\\nprepare your custom datasets to use when experimenting with these\\ngenerative models. Once you are ready to scale your efforts to a larger\\ndistributed cluster, you would then migrate to SageMaker distributed\\ntraining jobs to scale to a larger compute cluster using accelerators like\\nthe NVIDIA GPU or AWS Trainium, as you will see in Chapter 4.\\nWhile you may be able to avoid accelerators initially, you will very\\nlikely need to use them for longer-term development and deployment of\\nmore complex models. The sooner you learn the unique—and\\nsometimes obscure—aspects of developing with accelerators like\\nNVIDIA GPUs or AWS Trainium chips, the better. Fortunately, a lot of\\nthe complexity has been abstracted by the hardware provider through\\nthe NVIDIA CUDA library and AWS Neuron SDK, respectively.\\nAdapt, align, and augment.\\nIt’s important to adapt generative models to your specific domain, use\\ncase, and task. Chapters 5, 6, 7, and 11 are dedicated to fine-tuning your\\nmultimodal generative AI models with your custom datasets to meet\\nyour business goals.\\nAdditionally, as these generative models become more and more\\nhumanlike, it is important that they align with human values and\\npreferences—and, in general, behave well. Chapters 7 and 11 explore a\\ntechnique called reinforcement learning from human feedback (RLHF)\\nto align your multimodal generative models to be more helpful, honest,\\nand harmless (HHH). RLHF is a key component of the much-broader\\nfield of responsible AI.\\nWhile generative models contain an enormous amount of information\\nand knowledge, they often need to be augmented with current news or\\nproprietary data for your business. In Chapter 9, you will explore ways\\nto augment your generative models with external data sources or APIs.\\nEvaluate.\\nTo properly implement generative AI applications, you need to iterate\\nheavily. Therefore, it’s important to establish well-defined evaluation\\nmetrics and benchmarks to help measure the effectiveness of fine\\x02tuning. You will learn about model evaluation in Chapter 5. While not\\nas straightforward as traditional machine learning, model evaluation\\nhelps measure improvements to your models during the adaptation and\\nalignment phase—specifically, how well the model aligns to your\\nbusiness goals and human preferences.\\nDeploy and integrate.\\nWhen you finally have a well-tuned and aligned generative model, it’s\\ntime to deploy your model for inference and integrate the model into\\nyour application. In Chapter 8, you will see how to optimize the model\\nfor inference and better utilize your compute resources, reduce\\ninference latency, and delight your users.\\nYou will also see how to deploy your models with the AWS Inferentia\\nfamily of compute instances optimized for generative inference using\\nAmazon SageMaker endpoints. SageMaker endpoints are a great option\\nfor serving generative models as they are highly scalable, fault tolerant,\\nand customizable. They offer flexible deployment and scaling options\\nlike A/B testing, shadow deployments, and autoscaling, as you will\\nlearn in Chapter 8.\\nMonitor.\\nAs with any production system, you should set up proper metrics\\ncollection and monitoring systems for all components of your\\ngenerative AI application. In Chapters 8 and 12, you will learn how to\\nutilize Amazon CloudWatch and CloudTrail to monitor your generative\\nAI applications running on AWS. These services are highly\\ncustomizable, accessible from the AWS console or AWS software\\ndevelopment kit (SDK), and integrated with every AWS service\\nincluding Amazon Bedrock, a managed service for generative AI, which\\nyou will explore in Chapter 12.\\nGenerative AI on AWS\\nThis section will outline the AWS stack of purpose-built generative AI\\nservices and features, as shown in Figure 1-4, as well as discuss some of the\\nbenefits of using AWS for generative AI.\\nFigure 1-4. AWS services and features supporting generative AI\\nModel providers include those that are building or pretraining foundation\\nmodels requiring access to powerful, and cost performant, compute and\\nstorage resources. For this, AWS offers a range of frameworks and\\ninfrastructure to build foundation models. This includes optimized compute\\ninstances for generative AI with self-managed options such as Amazon EC2\\nas well as managed options like Amazon SageMaker for model training and\\nmodel deployment. In addition, AWS offers its own accelerators optimized\\nfor training (AWS Trainium) and deploying generative models (AWS\\nInferentia).\\nAWS Trainium is an accelerator that is purpose-built for high-performance,\\nlow-cost training workloads. Similarly, AWS Inferentia is purpose-built for\\nhigh-throughput, low-cost inference. The infrastructure options on AWS\\nthat are optimized for generative AI are used by model providers but also\\nmodel tuners.\\nModel tuners include those that are adapting or aligning foundation models\\nto their specific domain, use case, and task. This typically requires access to\\nnot only storage and compute resources but also tooling that helps enable\\nthese tasks through easy access to a range of foundation models while\\nremoving the need to manage underlying infrastructure. In addition to the\\nrange of optimized infrastructure available on AWS, tuners also have access\\nto a broad range of popular foundation models as well as tooling to adapt or\\nalign foundation models, including capabilities built into Amazon Bedrock\\nand Amazon SageMaker JumpStart.\\nAmazon Bedrock is a fully managed service that provides access to models\\nfrom Amazon (e.g., Titan) and popular third-party providers (e.g., AI21\\nLabs, Anthropic, Cohere, and Stability AI). This allows you to quickly get\\nstarted experimenting with available foundation models. Bedrock also\\nallows you to privately customize foundation models with your own data as\\nwell as integrate and deploy those models into generative AI applications.\\nAgents for Bedrock are fully managed and allow for additional\\ncustomization with the integration of proprietary external data sources and\\nthe ability to complete tasks.\\nAmazon SageMaker JumpStart provides access to both public and\\nproprietary foundation models through a model hub that includes the ability\\nto easily deploy a foundation model to Amazon SageMaker model\\ndeployment real-time endpoints. Additionally, SageMaker JumpStart\\nprovides the ability to fine-tune available models utilizing SageMaker\\nmodel training. SageMaker JumpStart automatically generates notebooks\\nwith code for deploying and fine-tuning models available on the model hub.\\nAmazon SageMaker provides additional extensibility, through managed\\nenvironments in Amazon SageMaker Studio notebooks, to work with any\\navailable foundation model, regardless of whether it’s available in\\nSageMaker JumpStart. As a result, you have the ability to work with any\\nmodels accessible to you and are never limited in the models you can work\\nwith in Amazon SageMaker.\\nAdapting a model to a specific use case, task, or domain often includes\\naugmenting the model with additional data. AWS also provides multiple\\nimplementation options for vector stores that store vector embeddings.\\nVector stores and embeddings are used for retrieval-augmented generation\\n(RAG) to efficiently retrieve relevant information from external data\\nsources to augment the data used with a generative model.\\nThe options available include vector engine for Amazon OpenSearch\\nServerless as well as the k-NN plugin available for use with Amazon\\nOpenSearch Service. In addition, both Amazon Aurora PostgreSQL and\\nAmazon Relational Database Services (RDS) for PostgreSQL include\\nvector stores capabilities through built-in pgvector support.\\nIf you are looking for a fully managed semantic search experience on\\ndomain-specific data, you can use Amazon Kendra, which creates and\\nmanages the embeddings for you.\\nAWS offers multiple options if you want to access generative models\\nthrough end-to-end generative AI applications. On AWS, you can build\\nyour own custom generative AI applications using the breadth and depth of\\nservices available; you can also take advantage of packaged, fully managed,\\nservices.\\nFor example, Amazon CodeWhisperer provides generative coding\\ncapabilities across multiple coding languages, supporting productivity\\nenhancements such as code generation, proactively scanning for\\nvulnerabilities and suggesting code remediations, with automatic\\nsuggestions for code attribution.\\nAWS HealthScribe is another packaged generative AI service targeted\\ntoward the healthcare industry to allow for the automatic generation of\\nclinical notes based on patient-clinician conversations.\\nFinally, Amazon QuickSight Q includes built-in generative capabilities\\nallowing users to ask questions about data in natural language and receive\\nanswers as well as generated visualizations that allow users to gain more\\ninsights into their data.\\nThis book will largely focus on the personas and tasks involved in the\\nsection “Generative AI Project Life Cycle”—as well as building generative\\nAI applications. Many of the services highlighted in this section, such as\\nAmazon SageMaker JumpStart and Amazon Bedrock, will be referenced\\nthroughout this book as you dive into specific areas of the generative AI\\nproject life cycle.\\nNow that we’ve introduced some core AWS services for generative AI, let’s\\nlook at some of the benefits of using AWS to build generative AI\\napplications.\\nWhy Generative AI on AWS?\\nKey benefits of utilizing AWS for your generative AI workloads include\\nincreased flexibility and choice, enterprise-grade security and governance\\ncapabilities, state-of-the art generative AI capabilities, low operational\\noverhead through fully managed services, the ability to quickly get started\\nwith ready-to-use solutions and services, and a strong history of continuous\\ninnovation. Let’s dive a bit further into each of these with some specific\\nexamples:\\nIncreased flexibility and choice\\nAWS provides flexibility not only in the ability to utilize a range of\\nservices and features to meet the needs of each use case, but also in\\nterms of choice in generative models. This provides you with the ability\\nto not only choose the right model for a use case, but to also change and\\ncontinually evaluate new models to take advantage of new capabilities.\\nEnterprise-grade security and governance capabilities\\nAWS services are built with security and governance capabilities that\\nare important to the most regulated industries. For example, SageMaker\\nmodel training, SageMaker model deployment, and Amazon Bedrock\\nsupport key capabilities around data protection, network isolation,\\ncontrolled access and authorization, as well as threat detection.\\nState-of-the-art generative AI capabilities\\nAWS offers choice in generative AI models, from Amazon models as\\nwell as third-party provider models in Amazon Bedrock to open source\\nand proprietary models offered through Amazon SageMaker JumpStart.\\nAdditionally, AWS has also invested in infrastructure like AWS\\nTrainium and AWS Inferentia for training and deploying generative\\nmodels at scale.\\nLow operational overhead\\nAs previously discussed, many of the AWS services and features\\ntargeted toward generative AI are offered through managed\\ninfrastructure, serverless offerings, or packaged solutions. This allows\\nyou to focus on generative AI models and applications instead of\\nmanaging infrastructure and to quickly get started with ready-to-use\\nsolutions and services.\\nStrong history of continuous innovation\\nAWS has an established history of rapid innovation built on years of\\nexperience in not only cloud infrastructure but artificial intelligence.\\nThe AWS stack of services and features for supporting generative AI covers\\nthe breadth, depth, and extensibility to support every use case, whether\\nyou’re a model provider, a tuner, or a consumer. In addition to the\\ngenerative AI capabilities on AWS, a broader set of AWS services also\\nsupports the ability to build custom generative AI applications, which will\\nbe covered in the next section.\\nBuilding Generative AI Applications on AWS\\nA generative AI application includes more than generative models. It\\nrequires multiple components to build reliable, scalable, and secure\\napplications that are then offered to consumers of that application, whether\\nthey are end users or other systems, as shown in Figure 1-5.\\nFigure 1-5. Generative AI applications include more than foundation models\\nWhen using a packaged generative AI service such as Amazon\\nCodeWhisperer, all of this is completely abstracted and provided to the end\\nuser. However, building custom generative AI applications typically\\nrequires a range of services. AWS provides the breadth of services that are\\noften required to build an end-to-end generative AI application. Figure 1-6\\nshows an example of AWS services that may be used as part of a broader\\ngenerative AI application.\\nFigure 1-6. AWS breadth of service to enable customers to build generative AI\\napplications\\nSummary\\nIn this chapter, you explored some common generative AI use cases and\\nlearned some generative AI fundamentals. You also saw an example of a\\ntypical generative AI project life cycle that includes various stages,\\nincluding defining a use case, prompt engineering (Chapter 2), selecting a\\nfoundation model (Chapter 3), fine-tuning (Chapters 5 and 6), aligning with\\nhuman values (Chapter 7), deploying your model (Chapter 8), and\\nintegrating with external data sources and agents (Chapter 9).\\nThe compute-intensive parts of the life cycle—including fine-tuning and\\nhuman alignment—will benefit from an understanding of quantization and\\ndistributed-computing algorithms (Chapter 4). These optimizations and\\nalgorithms will speed up the iterative development cycle that is critical\\nwhen developing generative AI models.\\nIn Chapter 2, you will learn some prompt engineering tips and best\\npractices. These are useful for prompting both language-only foundation\\nmodels (Chapter 3) and multimodal foundation models (Chapters 10 and\\n11) using either Amazon SageMaker JumpStart model hub (Chapter 3) or\\nthe Amazon Bedrock managed generative AI service (Chapter 12)'), type='text')], created_at=1708701150, file_ids=[], metadata={}, object='thread.message', role='user', run_id=None, thread_id='thread_OwC2Nx9eHHZtv15YUdxSPppv')], object='list', first_id='msg_IdTKRL9vmmCEPAQaQi6sZRM9', last_id='msg_Z5CAQuYTW8ewa0oDIJ3hxEs9', has_more=False)\n"
     ]
    }
   ],
   "source": [
    "print(get_last_message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pretty_print(messages) -> None:\n",
    "    print(\"# Messages\")\n",
    "    for message in messages.data:\n",
    "        role_label = \"User\" if message.role == \"user\" else \"Assistant\"\n",
    "        # Check the type of message content and handle accordingly\n",
    "        for content in message.content:\n",
    "            if content.type == \"text\":\n",
    "                message_content = content.text.value\n",
    "                print(f\"{role_label}: {message_content}\\n\")\n",
    "                print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Messages\n",
      "Assistant: Based on the provided content about Applications of Generative AI for Businesses, I have generated 10 quiz questions that cover fundamental concepts about Generative AI. The questions are of three types: single_select_mcq, multi_select_mcq, and open_text_question. Here are the quiz questions:\n",
      "\n",
      "1. **Question 1 (single_select_mcq):**\n",
      "   - **Time Limit:** 1 minute\n",
      "   - **Difficulty:** Easy\n",
      "   - **Question:** What is one of the common use cases of generative AI mentioned in the provided content?\n",
      "   - **Options:**\n",
      "     1. Text summarization\n",
      "     2. Image classification\n",
      "     3. Sentiment analysis\n",
      "     4. Speech recognition\n",
      "\n",
      "2. **Question 2 (multi_select_mcq):**\n",
      "   - **Time Limit:** 1 minute\n",
      "   - **Difficulty:** Easy\n",
      "   - **Question:** Select the tasks associated with generative AI use cases from the following options:\n",
      "   - **Options:**\n",
      "     1. Text summarization\n",
      "     2. Image generation\n",
      "     3. Classification and content moderation\n",
      "     4. Conversational interface\n",
      "\n",
      "3. **Question 3 (open_text_question):**\n",
      "   - **Time Limit:** 1 minute\n",
      "   - **Difficulty:** Easy\n",
      "   - **Question:** Explain the concept of \"Question answering (QA) and visual question answering (VQA)\" as mentioned in the content.\n",
      "\n",
      "4. **Question 4 (single_select_mcq):**\n",
      "   - **Time Limit:** 1 minute\n",
      "   - **Difficulty:** Easy\n",
      "   - **Question:** In generative AI, what task involves handling multiturn conversations through a chat-like interface?\n",
      "   - **Options:**\n",
      "     1. Classification and content moderation\n",
      "     2. Conversational interface\n",
      "     3. Text summarization\n",
      "     4. Source code generation\n",
      "\n",
      "5. **Question 5 (multi_select_mcq):**\n",
      "   - **Time Limit:** 1 minute\n",
      "   - **Difficulty:** Easy\n",
      "   - **Question:** Select the examples of generative AI use cases mentioned in the content:\n",
      "   - **Options:**\n",
      "     1. Producing personalized marketing\n",
      "     2. Editing software code\n",
      "     3. Detecting harmful content\n",
      "     4. Summarizing news articles\n",
      "\n",
      "6. **Question 6 (open_text_question):**\n",
      "   - **Time Limit:** 1 minute\n",
      "   - **Difficulty:** Easy\n",
      "   - **Question:** Describe the importance of fine-tuning in generative AI projects based on the provided content.\n",
      "\n",
      "7. **Question 7 (single_select_mcq):**\n",
      "   - **Time Limit:** 1 minute\n",
      "   - **Difficulty:** Easy\n",
      "   - **Question:** What does generative AI help in extracting from documents, such as names, addresses, events, or numeric data?\n",
      "   - **Options:**\n",
      "     1. Toxic content\n",
      "     2. Personally identifiable information\n",
      "     3. Visual information\n",
      "     4. Audio clips\n",
      "\n",
      "8. **Question 8 (multi_select_mcq):**\n",
      "   - **Time Limit:** 1 minute\n",
      "   - **Difficulty:** Easy\n",
      "   - **Question:** Which of the following tasks are associated with an extension to the question-answer task in generative AI?\n",
      "   - **Options:**\n",
      "     1. Classification and content moderation\n",
      "     2. Masking personally identifiable information\n",
      "     3. Detecting toxic or harmful content\n",
      "     4. Reasoning through problems\n",
      "\n",
      "9. **Question 9 (open_text_question):**\n",
      "   - **Time Limit:** 1 minute\n",
      "   - **Difficulty:** Easy\n",
      "   - **Question:** Discuss the significance of adapting generative models to specific domains, as mentioned in the content.\n",
      "\n",
      "10. **Question 10 (single_select_mcq):**\n",
      "   - **Time Limit:** 1 minute\n",
      "   - **Difficulty:** Easy\n",
      "   - **Question:** What does Amazon SageMaker JumpStart provide access to in the generative AI context?\n",
      "   - **Options:**\n",
      "     1. Foundation models\n",
      "     2. Hardware accelerators\n",
      "     3. Enterprise software solutions\n",
      "     4. Data storage services\n",
      "\n",
      "These quiz questions aim to test understanding and knowledge about the applications and fundamentals of Generative AI for businesses.\n",
      "\n",
      "\n",
      "User: Create a 30 minutes \"Generative AI\" that will have 10 Questions. The Questions type will include: 1. \"single_select_mcq\" 2. \"multi_select_mcq\" and \"open_text_question\". The Quiz will be easy and the questions generated shall follow: \"Create a quiz about Applications of Generative AI for Businesses. It will conver fundamental concepts about it. The Content from which Quiz Shall be generated is: \"Chapter 1. Generative AI Use Cases,\n",
      "Fundamentals, and Project Life Cycle\n",
      "In this chapter, you will see some generative AI tasks and use cases in\n",
      "action, gain an understanding of generative foundation models, and explore\n",
      "a typical generative AI project life cycle. The use cases and tasks you’ll see\n",
      "in this chapter include intelligent search, automated customer-support\n",
      "chatbot, dialog summarization, not-safe-for-work (NSFW) content\n",
      "moderation, personalized product videos, source code generation, and\n",
      "others.\n",
      "You will also learn a few of the generative AI service and hardware options\n",
      "from Amazon Web Services (AWS) including Amazon Bedrock, Amazon\n",
      "SageMaker, Amazon CodeWhisperer, AWS Trainium, and AWS Inferentia.\n",
      "These service and hardware options provide great flexibility when building\n",
      "your end-to-end, context-aware, multimodal reasoning applications with\n",
      "generative AI on AWS.\n",
      "Let’s explore some common use cases and tasks for generative AI.\n",
      "Use Cases and Tasks\n",
      "Similar to deep learning, generative AI is a general-purpose technology\n",
      "used for multiple purposes across many industries and customer segments.\n",
      "There are many types of multimodal generative AI tasks. We’ve included a\n",
      "list of the most common generative tasks and associated example use cases:\n",
      "Text summarization\n",
      "Produce a shorter version of a piece of text while retaining the main\n",
      "ideas. Examples include summarizing a news article, legal document, or\n",
      "financial report into a smaller number of words or paragraphs for faster\n",
      "consumption. Often, summarization is used on customer support\n",
      "conversations to provide a quick overview of the interaction between a\n",
      "customer and support representative.\n",
      "Rewriting\n",
      "Modify the wording of a piece of text to adapt to a different audience,\n",
      "formality, or tone. For example, you can convert a formal legal\n",
      "document into a less formal document using less legal terms to appeal\n",
      "to a nonlegal audience.\n",
      "Information extraction\n",
      "Extract information from documents such as names, addresses, events,\n",
      "or numeric data or numbers. For example, converting an email into a\n",
      "purchase order in an enterprise resource planning (ERP) system like\n",
      "SAP.\n",
      "Question answering (QA) and visual question answering (VQA)\n",
      "Ask questions directly against a set of documents, images, videos, or\n",
      "audio clips. For example, you can set up an internal, employee-facing\n",
      "chatbot to answer questions about human resources and benefits\n",
      "documents.\n",
      "Detecting toxic or harmful content\n",
      "An extension to the question-answer task, you can ask a generative\n",
      "model if a set of text, images, videos, or audio clips contains any\n",
      "toxicity or harmful content.\n",
      "Classification and content moderation\n",
      "Assign a category to a given piece of content such as a document,\n",
      "image, video, or audio clip. For example, deleting email spam, filtering\n",
      "out inappropriate images, or labeling incoming, text-based customer\u0002support tickets.\n",
      "Conversational interface\n",
      "Handle multiturn conversations to accomplish tasks through a chat-like\n",
      "interface. Examples include chatbots for self-service customer support\n",
      "or mental health therapy sessions.\n",
      "Translation\n",
      "One of the earliest use cases for generative AI is language translation.\n",
      "Consider, for example, that the publisher of this book wants to release a\n",
      "German translation to help expand the book’s reach. Or perhaps you\n",
      "may want to convert the Python-based examples to Java to work within\n",
      "your existing Java-based enterprise application.\n",
      "Source code generation\n",
      "Create source code from natural language code comments—or even a\n",
      "hand-drawn sketch, as shown in Figure 1-1. Here, an HTML- and\n",
      "JavaScript-based website is generated from a UI sketch scribbled on the\n",
      "back of a restaurant napkin.\n",
      "Figure 1-1. Generating UI code from hand-drawn sketch\n",
      "Reasoning\n",
      "Reason through a problem to discover potential new solutions, trade\u0002offs, or latent details. For example, consider a CFO who provides an\n",
      "audio-based quarterly financial report to investors as well as a more\u0002detailed written report. By reasoning through these different media\n",
      "formats together, the model may discover some conclusions about the\n",
      "company’s health not directly mentioned in the audio or stated in the\n",
      "text.\n",
      "Mask personally identifiable information (PII)\n",
      "You can use generative models to mask personally identifiable\n",
      "information from a given corpus of text. This is useful for many use\n",
      "cases where you are working with sensitive data and wish to remove PII\n",
      "data from your workflows.\n",
      "Personalized marketing and ads\n",
      "Generate personalized product descriptions, videos, or ads based on user\n",
      "profile features. Consider an ecommerce website that wants to generate\n",
      "a personalized description for each product based on the logged-in\n",
      "user’s age or family situation. You could also generate personalized\n",
      "product images that include mature adults, adults with children, or\n",
      "children themselves to better appeal to the logged-in user’s\n",
      "demographic, as shown in Figure 1-2.\n",
      "Figure 1-2. Personalized marketing\n",
      "In this case, each user of the service would potentially see a unique and\n",
      "highly personalized image and description for the same product. This\n",
      "could ultimately lead to more product clicks and higher sales.\n",
      "In each of these generative use cases and tasks, a model creates content that\n",
      "approximates a human’s understanding of language. This is truly amazing\n",
      "and is made possible by a neural network architecture called the\n",
      "transformer, which you will learn in Chapter 3.\n",
      "In the next section, you will learn how to access foundation models through\n",
      "model hubs.\n",
      "Foundation Models and Model Hubs\n",
      "Foundation models are very large and complex neural network models\n",
      "consisting of billions of parameters (a.k.a. weights). The model parameters\n",
      "are learned during the training phase—often called pretraining. Foundation\n",
      "models are trained on massive amounts of training data—typically over a\n",
      "period of many weeks and months using large, distributed clusters of CPUs\n",
      "and graphics processing units (GPUs). After learning billions of parameters,\n",
      "these foundation models can represent complex entities such as human\n",
      "language, images, videos, and audio clips.\n",
      "In most cases, you will start your generative AI projects with an existing\n",
      "foundation model from a model hub such as Hugging Face Model Hub,\n",
      "PyTorch Hub, or Amazon SageMaker JumpStart. A model hub is a\n",
      "collection of models that typically contains detailed model descriptions\n",
      "including the use cases that they address.\n",
      "Throughout this book, we will use Hugging Face Model Hub and\n",
      "SageMaker JumpStart to access foundation models like Llama 2 from Meta\n",
      "(Facebook) and Falcon from the Technology Innovation Institute (TII) and\n",
      "FLAN-T5 from Google. You will dive deeper into model hubs and\n",
      "foundation models in Chapter 3.\n",
      "Next, you’ll see a typical generative AI project life cycle that roughly\n",
      "follows the outline of the rest of this book.\n",
      "Generative AI Project Life Cycle\n",
      "While there is no definitive project life cycle for generative AI projects, the\n",
      "framework shown in Figure 1-3 can help guide you through the most\n",
      "important parts of your generative AI application journey. Throughout the\n",
      "book, you will gain intuition, learn to avoid potential difficulties, and\n",
      "improve your decision making at each step in the journey.\n",
      "Figure 1-3. Generative AI project life cycle framework\n",
      "Let’s dive into each component of the life cycle shown in Figure 1-3:\n",
      "Identify use case.\n",
      "As with any project, you first want to define your scope, including the\n",
      "specific generative use case and task that you plan to address with your\n",
      "generative AI application. We recommend that you start with a single,\n",
      "well-documented generative use case. This will help you get familiar\n",
      "with the environment and understand the power—and limitations—of\n",
      "these models without trying to optimize the model for different tasks at\n",
      "the same time. While these models are capable of carrying out multiple\n",
      "tasks, it’s a bit more difficult to evaluate and optimize the model across\n",
      "multiple tasks to start.\n",
      "Experiment and select.\n",
      "Generative AI models are capable of carrying out many different tasks\n",
      "with great success. However, you will need to decide if an existing\n",
      "foundation model is suitable for your application needs. In Chapter 2,\n",
      "you will learn how to work with these existing foundation models right\n",
      "out of the box using techniques called prompt engineering and in\u0002context learning.\n",
      "Most commonly, you will start from an existing foundation model (as\n",
      "you will see in Chapter 3). This will greatly improve your time-to\u0002market since you will avoid the pretraining step, which is extremely\n",
      "resource intensive and often requires trillions of words, images, videos,\n",
      "or audio clips to get started. Operating at this scale requires a lot of\n",
      "time, patience, and compute—often millions of GPU hours are required\n",
      "when pretraining from scratch.\n",
      "You also want to consider the size of the foundation model you decide\n",
      "to work with as this will impact the hardware—and cost—needed to\n",
      "train and serve your models. While larger models tend to generalize\n",
      "better to more tasks, this is not always the case and depends on the\n",
      "dataset used during training and tuning.\n",
      "We recommend that you try different models for your generative use\n",
      "case and task. Start with an existing, well-documented, relatively small\n",
      "(e.g., 7 billion-parameter) foundation model to iterate quickly and learn\n",
      "the unique ways of interacting with these generative AI models with a\n",
      "relatively small amount of hardware (compared to the larger 175+\n",
      "billion-parameter models).\n",
      "During development, you would typically start with a playground\n",
      "environment within either Amazon SageMaker JumpStart or Amazon\n",
      "Bedrock. This lets you try different prompts and models quickly, as you\n",
      "will see in Chapter 2. Next, you might use a Jupyter notebook or Python\n",
      "script using an integrated development environment (IDE) like Visual\n",
      "Studio Code (VS Code) or Amazon SageMaker Studio notebooks to\n",
      "prepare your custom datasets to use when experimenting with these\n",
      "generative models. Once you are ready to scale your efforts to a larger\n",
      "distributed cluster, you would then migrate to SageMaker distributed\n",
      "training jobs to scale to a larger compute cluster using accelerators like\n",
      "the NVIDIA GPU or AWS Trainium, as you will see in Chapter 4.\n",
      "While you may be able to avoid accelerators initially, you will very\n",
      "likely need to use them for longer-term development and deployment of\n",
      "more complex models. The sooner you learn the unique—and\n",
      "sometimes obscure—aspects of developing with accelerators like\n",
      "NVIDIA GPUs or AWS Trainium chips, the better. Fortunately, a lot of\n",
      "the complexity has been abstracted by the hardware provider through\n",
      "the NVIDIA CUDA library and AWS Neuron SDK, respectively.\n",
      "Adapt, align, and augment.\n",
      "It’s important to adapt generative models to your specific domain, use\n",
      "case, and task. Chapters 5, 6, 7, and 11 are dedicated to fine-tuning your\n",
      "multimodal generative AI models with your custom datasets to meet\n",
      "your business goals.\n",
      "Additionally, as these generative models become more and more\n",
      "humanlike, it is important that they align with human values and\n",
      "preferences—and, in general, behave well. Chapters 7 and 11 explore a\n",
      "technique called reinforcement learning from human feedback (RLHF)\n",
      "to align your multimodal generative models to be more helpful, honest,\n",
      "and harmless (HHH). RLHF is a key component of the much-broader\n",
      "field of responsible AI.\n",
      "While generative models contain an enormous amount of information\n",
      "and knowledge, they often need to be augmented with current news or\n",
      "proprietary data for your business. In Chapter 9, you will explore ways\n",
      "to augment your generative models with external data sources or APIs.\n",
      "Evaluate.\n",
      "To properly implement generative AI applications, you need to iterate\n",
      "heavily. Therefore, it’s important to establish well-defined evaluation\n",
      "metrics and benchmarks to help measure the effectiveness of fine\u0002tuning. You will learn about model evaluation in Chapter 5. While not\n",
      "as straightforward as traditional machine learning, model evaluation\n",
      "helps measure improvements to your models during the adaptation and\n",
      "alignment phase—specifically, how well the model aligns to your\n",
      "business goals and human preferences.\n",
      "Deploy and integrate.\n",
      "When you finally have a well-tuned and aligned generative model, it’s\n",
      "time to deploy your model for inference and integrate the model into\n",
      "your application. In Chapter 8, you will see how to optimize the model\n",
      "for inference and better utilize your compute resources, reduce\n",
      "inference latency, and delight your users.\n",
      "You will also see how to deploy your models with the AWS Inferentia\n",
      "family of compute instances optimized for generative inference using\n",
      "Amazon SageMaker endpoints. SageMaker endpoints are a great option\n",
      "for serving generative models as they are highly scalable, fault tolerant,\n",
      "and customizable. They offer flexible deployment and scaling options\n",
      "like A/B testing, shadow deployments, and autoscaling, as you will\n",
      "learn in Chapter 8.\n",
      "Monitor.\n",
      "As with any production system, you should set up proper metrics\n",
      "collection and monitoring systems for all components of your\n",
      "generative AI application. In Chapters 8 and 12, you will learn how to\n",
      "utilize Amazon CloudWatch and CloudTrail to monitor your generative\n",
      "AI applications running on AWS. These services are highly\n",
      "customizable, accessible from the AWS console or AWS software\n",
      "development kit (SDK), and integrated with every AWS service\n",
      "including Amazon Bedrock, a managed service for generative AI, which\n",
      "you will explore in Chapter 12.\n",
      "Generative AI on AWS\n",
      "This section will outline the AWS stack of purpose-built generative AI\n",
      "services and features, as shown in Figure 1-4, as well as discuss some of the\n",
      "benefits of using AWS for generative AI.\n",
      "Figure 1-4. AWS services and features supporting generative AI\n",
      "Model providers include those that are building or pretraining foundation\n",
      "models requiring access to powerful, and cost performant, compute and\n",
      "storage resources. For this, AWS offers a range of frameworks and\n",
      "infrastructure to build foundation models. This includes optimized compute\n",
      "instances for generative AI with self-managed options such as Amazon EC2\n",
      "as well as managed options like Amazon SageMaker for model training and\n",
      "model deployment. In addition, AWS offers its own accelerators optimized\n",
      "for training (AWS Trainium) and deploying generative models (AWS\n",
      "Inferentia).\n",
      "AWS Trainium is an accelerator that is purpose-built for high-performance,\n",
      "low-cost training workloads. Similarly, AWS Inferentia is purpose-built for\n",
      "high-throughput, low-cost inference. The infrastructure options on AWS\n",
      "that are optimized for generative AI are used by model providers but also\n",
      "model tuners.\n",
      "Model tuners include those that are adapting or aligning foundation models\n",
      "to their specific domain, use case, and task. This typically requires access to\n",
      "not only storage and compute resources but also tooling that helps enable\n",
      "these tasks through easy access to a range of foundation models while\n",
      "removing the need to manage underlying infrastructure. In addition to the\n",
      "range of optimized infrastructure available on AWS, tuners also have access\n",
      "to a broad range of popular foundation models as well as tooling to adapt or\n",
      "align foundation models, including capabilities built into Amazon Bedrock\n",
      "and Amazon SageMaker JumpStart.\n",
      "Amazon Bedrock is a fully managed service that provides access to models\n",
      "from Amazon (e.g., Titan) and popular third-party providers (e.g., AI21\n",
      "Labs, Anthropic, Cohere, and Stability AI). This allows you to quickly get\n",
      "started experimenting with available foundation models. Bedrock also\n",
      "allows you to privately customize foundation models with your own data as\n",
      "well as integrate and deploy those models into generative AI applications.\n",
      "Agents for Bedrock are fully managed and allow for additional\n",
      "customization with the integration of proprietary external data sources and\n",
      "the ability to complete tasks.\n",
      "Amazon SageMaker JumpStart provides access to both public and\n",
      "proprietary foundation models through a model hub that includes the ability\n",
      "to easily deploy a foundation model to Amazon SageMaker model\n",
      "deployment real-time endpoints. Additionally, SageMaker JumpStart\n",
      "provides the ability to fine-tune available models utilizing SageMaker\n",
      "model training. SageMaker JumpStart automatically generates notebooks\n",
      "with code for deploying and fine-tuning models available on the model hub.\n",
      "Amazon SageMaker provides additional extensibility, through managed\n",
      "environments in Amazon SageMaker Studio notebooks, to work with any\n",
      "available foundation model, regardless of whether it’s available in\n",
      "SageMaker JumpStart. As a result, you have the ability to work with any\n",
      "models accessible to you and are never limited in the models you can work\n",
      "with in Amazon SageMaker.\n",
      "Adapting a model to a specific use case, task, or domain often includes\n",
      "augmenting the model with additional data. AWS also provides multiple\n",
      "implementation options for vector stores that store vector embeddings.\n",
      "Vector stores and embeddings are used for retrieval-augmented generation\n",
      "(RAG) to efficiently retrieve relevant information from external data\n",
      "sources to augment the data used with a generative model.\n",
      "The options available include vector engine for Amazon OpenSearch\n",
      "Serverless as well as the k-NN plugin available for use with Amazon\n",
      "OpenSearch Service. In addition, both Amazon Aurora PostgreSQL and\n",
      "Amazon Relational Database Services (RDS) for PostgreSQL include\n",
      "vector stores capabilities through built-in pgvector support.\n",
      "If you are looking for a fully managed semantic search experience on\n",
      "domain-specific data, you can use Amazon Kendra, which creates and\n",
      "manages the embeddings for you.\n",
      "AWS offers multiple options if you want to access generative models\n",
      "through end-to-end generative AI applications. On AWS, you can build\n",
      "your own custom generative AI applications using the breadth and depth of\n",
      "services available; you can also take advantage of packaged, fully managed,\n",
      "services.\n",
      "For example, Amazon CodeWhisperer provides generative coding\n",
      "capabilities across multiple coding languages, supporting productivity\n",
      "enhancements such as code generation, proactively scanning for\n",
      "vulnerabilities and suggesting code remediations, with automatic\n",
      "suggestions for code attribution.\n",
      "AWS HealthScribe is another packaged generative AI service targeted\n",
      "toward the healthcare industry to allow for the automatic generation of\n",
      "clinical notes based on patient-clinician conversations.\n",
      "Finally, Amazon QuickSight Q includes built-in generative capabilities\n",
      "allowing users to ask questions about data in natural language and receive\n",
      "answers as well as generated visualizations that allow users to gain more\n",
      "insights into their data.\n",
      "This book will largely focus on the personas and tasks involved in the\n",
      "section “Generative AI Project Life Cycle”—as well as building generative\n",
      "AI applications. Many of the services highlighted in this section, such as\n",
      "Amazon SageMaker JumpStart and Amazon Bedrock, will be referenced\n",
      "throughout this book as you dive into specific areas of the generative AI\n",
      "project life cycle.\n",
      "Now that we’ve introduced some core AWS services for generative AI, let’s\n",
      "look at some of the benefits of using AWS to build generative AI\n",
      "applications.\n",
      "Why Generative AI on AWS?\n",
      "Key benefits of utilizing AWS for your generative AI workloads include\n",
      "increased flexibility and choice, enterprise-grade security and governance\n",
      "capabilities, state-of-the art generative AI capabilities, low operational\n",
      "overhead through fully managed services, the ability to quickly get started\n",
      "with ready-to-use solutions and services, and a strong history of continuous\n",
      "innovation. Let’s dive a bit further into each of these with some specific\n",
      "examples:\n",
      "Increased flexibility and choice\n",
      "AWS provides flexibility not only in the ability to utilize a range of\n",
      "services and features to meet the needs of each use case, but also in\n",
      "terms of choice in generative models. This provides you with the ability\n",
      "to not only choose the right model for a use case, but to also change and\n",
      "continually evaluate new models to take advantage of new capabilities.\n",
      "Enterprise-grade security and governance capabilities\n",
      "AWS services are built with security and governance capabilities that\n",
      "are important to the most regulated industries. For example, SageMaker\n",
      "model training, SageMaker model deployment, and Amazon Bedrock\n",
      "support key capabilities around data protection, network isolation,\n",
      "controlled access and authorization, as well as threat detection.\n",
      "State-of-the-art generative AI capabilities\n",
      "AWS offers choice in generative AI models, from Amazon models as\n",
      "well as third-party provider models in Amazon Bedrock to open source\n",
      "and proprietary models offered through Amazon SageMaker JumpStart.\n",
      "Additionally, AWS has also invested in infrastructure like AWS\n",
      "Trainium and AWS Inferentia for training and deploying generative\n",
      "models at scale.\n",
      "Low operational overhead\n",
      "As previously discussed, many of the AWS services and features\n",
      "targeted toward generative AI are offered through managed\n",
      "infrastructure, serverless offerings, or packaged solutions. This allows\n",
      "you to focus on generative AI models and applications instead of\n",
      "managing infrastructure and to quickly get started with ready-to-use\n",
      "solutions and services.\n",
      "Strong history of continuous innovation\n",
      "AWS has an established history of rapid innovation built on years of\n",
      "experience in not only cloud infrastructure but artificial intelligence.\n",
      "The AWS stack of services and features for supporting generative AI covers\n",
      "the breadth, depth, and extensibility to support every use case, whether\n",
      "you’re a model provider, a tuner, or a consumer. In addition to the\n",
      "generative AI capabilities on AWS, a broader set of AWS services also\n",
      "supports the ability to build custom generative AI applications, which will\n",
      "be covered in the next section.\n",
      "Building Generative AI Applications on AWS\n",
      "A generative AI application includes more than generative models. It\n",
      "requires multiple components to build reliable, scalable, and secure\n",
      "applications that are then offered to consumers of that application, whether\n",
      "they are end users or other systems, as shown in Figure 1-5.\n",
      "Figure 1-5. Generative AI applications include more than foundation models\n",
      "When using a packaged generative AI service such as Amazon\n",
      "CodeWhisperer, all of this is completely abstracted and provided to the end\n",
      "user. However, building custom generative AI applications typically\n",
      "requires a range of services. AWS provides the breadth of services that are\n",
      "often required to build an end-to-end generative AI application. Figure 1-6\n",
      "shows an example of AWS services that may be used as part of a broader\n",
      "generative AI application.\n",
      "Figure 1-6. AWS breadth of service to enable customers to build generative AI\n",
      "applications\n",
      "Summary\n",
      "In this chapter, you explored some common generative AI use cases and\n",
      "learned some generative AI fundamentals. You also saw an example of a\n",
      "typical generative AI project life cycle that includes various stages,\n",
      "including defining a use case, prompt engineering (Chapter 2), selecting a\n",
      "foundation model (Chapter 3), fine-tuning (Chapters 5 and 6), aligning with\n",
      "human values (Chapter 7), deploying your model (Chapter 8), and\n",
      "integrating with external data sources and agents (Chapter 9).\n",
      "The compute-intensive parts of the life cycle—including fine-tuning and\n",
      "human alignment—will benefit from an understanding of quantization and\n",
      "distributed-computing algorithms (Chapter 4). These optimizations and\n",
      "algorithms will speed up the iterative development cycle that is critical\n",
      "when developing generative AI models.\n",
      "In Chapter 2, you will learn some prompt engineering tips and best\n",
      "practices. These are useful for prompting both language-only foundation\n",
      "models (Chapter 3) and multimodal foundation models (Chapters 10 and\n",
      "11) using either Amazon SageMaker JumpStart model hub (Chapter 3) or\n",
      "the Amazon Bedrock managed generative AI service (Chapter 12)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pretty_print(get_last_message)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aix",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
